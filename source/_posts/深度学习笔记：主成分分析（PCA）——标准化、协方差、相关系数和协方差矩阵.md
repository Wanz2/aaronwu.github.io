---
title: 深度学习笔记：主成分分析（PCA）——标准化、协方差、相关系数和协方差矩阵
date: 2017-04-29 23:47:45
mathjax: true
tags: 机器学习
categories: 机器学习
description: 笔者在学习主成分分析（PCA）的时候接触到了协方差矩阵的应用。这部分知识有些遗忘了，因此重新巩固一下，记录在此，希望能帮助到有需要的同学。
---
&emsp;&emsp;笔者在学习主成分分析（PCA）的时候接触到了协方差矩阵的应用。这部分知识有些遗忘了，因此重新巩固一下，记录在此，希望能帮助到有需要的同学。  

<!-- more -->
#1. 概率论中的标准化、协方差、相关系数和协方差矩阵概念
## 1.1 随机变量的部分数字特征

&emsp;&emsp;假设有二维随机向量$(X,Y)$  

|数字特征|意义|描述|
|---|---|---|
|$E(X)$|数学期望|反映$X$的平均值|
|$D(X)$|方差|反映$X$与平均值偏离的程度|
|$Cov(X,Y)$|协方差|等于$E((X-E(X))(Y-E(Y)))$，若为0，则说明$X$$Y$独立|
|$\rho或\rho _{XY}$|相关系数（就是随机变量\*\*标准化\*\*后的协方差）|等于$\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}$|

## 1.2 随机变量的标准化
### 1.2.1 为什么要对随机变量进行标准化处理
&emsp;&emsp;随机变量的标准化，包含以下两点：  

1. 将随机变量的分布中心$E(X)$移至原点，不使分布中心偏左或偏右  

2. 缩小或扩大坐标轴，使分布不至于过疏或过密  

在排除了这些干扰以后，随机变量$X$的一些性质就会显露出来，便于我们进行进一步的分析。  
## 1.3 如何进行标准化处理
&emsp;&emsp;令随机变量$X$均值为0，方差为1。令$X^{\*}$和$Y^{\*}$分别表示标准化后的$X$和$Y$，则  

&emsp;&emsp;$X^{\*}=\frac{X-E(X)}{\sqrt{D(X)}}$，$Y^{\*}=\frac{Y-E(X)}{\sqrt{D(X)}}$  

而标准化后的$X^{\*}$和$Y^{\*}$的协方差就是\*\*相关系数，用$\rho$或$\rho_{XY}$表示\*\*，即  

&emsp;&emsp;$Cov(X^{\*},Y^{\*})=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}=\rho_{xy}$

## 1.4 相关系数的意义
&emsp;&emsp;通过上一节中随机变量的标准化，我们引出了相关系数，那么两个随机变量的相关系数有什么意义呢？  
&emsp;&emsp;结论：相关系数是对于随机变量相关性的度量：  

 - 当相关系数$\rho=1$时，随机变量$X$和$Y$之间存在线性关系，且为正线性相关  
 - 当相关系数$\rho=-1$时，两者之间为负线性关系  
 - $|\rho|\leq1$，线性相关性随着$|\rho|$的减小而减小。当$|\rho|=0$时，两者之间就不存在线性关系了  
 - 注意：  
  - 当$|\rho|=0$，随机变量$X$和$Y$是不线性相关的，但\*\*不能代表两者相互独立\*\*，他们之间可能存在别的相关关系；但当$X$和$Y$相互独立时，它们的相关系数$|\rho|=0$。可以说，\*\*$|\rho|=0$是$X$和$Y$相互独立的必要不充分条件。\*\*  

  - 但是，当随机变量$(X,Y)$服从二维正态分布时，则$X$和$Y$不相关等价于两者相互独立  

&emsp;&emsp;笔者在这里仅给出结论，因为本文仅仅是笔者在应用到相关知识点时的复习，为了理清思路而做的记录，关于上述结论的证明，可以在任意一本概率论的书中找到。  
## 1.5 协方差矩阵
&emsp;&emsp;令$(X\_1,X\_2,...,X\_n)$为$n$维随机向量($n\geq2$),记  

&emsp;&emsp;$b_{ij}=Cov(X_i,X_j)=E((X_i-E(X_i))(X_j-E(X_j))), i,j=1,2,...,n$，  

则矩阵  
$$
B=
\begin{bmatrix}
b_{11}&b_{12}&\cdots&b_{1n}\\
b_{21}&b_{22}&\cdots&b_{2n}\\
\vdots&\vdots&&\vdots\\
b_{n1}&b_{n2}&\cdots&b_{nn}
\end{bmatrix}
$$  

为$(X_1,\cdots,X_n)$的协方差矩阵。  

# 2.数理统计中的协方差和协方差矩阵概念
&emsp;&emsp;以上所说的是概率论中的协方差概念，但是我们在深度学习的实际运用中，通常是对已经获得的数据进行分析，因此类比概率论中的随机变量的数字特征，可以得到数理统计中的相关统计量，同时可以定义协方差和协方差矩阵  

## 2.1数理统计中的统计量
&emsp;&emsp;记$(X_1,X_2,\cdots,X_n)$是来自总体$X$的样本，$(x_1,x_2,\cdots,x_n)$是样本观察值。  

|统计量|意义|描述|
|---|---|---|
|$\bar{X}$|样本均值|$\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i$|
|$S^2$|样本方差|$S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2$|
|$S$|样本标准差|$S=\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2}$|  

## 2.2样本协方差
&emsp;&emsp;样本均值表征了样本分布的中间点；而样本标准差则是样本各个观察值到样本分布中间点的距离的平均值。样本均值和样本标准差均是用来描述一维数据的。  

&emsp;&emsp;但在生活中我们通常会用到多维数据，比如我们有两个总体$X$和$Y$，两者的样本分别是$(X_1,X_2,\cdots,X_n)$和$(Y_1,Y_2,\cdots,Y_n)$，样本观察值分别是$(x_1,x_2,\cdots,x_n)$和$(y_1,y_2,\cdots,y_n)$，我们希望能够分析出这两个样本的相关性，因此需要定义样本之间的协方差。回忆一下样本方差的定义：  

&emsp;&emsp;$S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2$  

仿照样本方差定义，我们可以定义样本协方差：  

&emsp;&emsp;$Cov(X,Y)=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})$  

## 2.3样本协方差矩阵
&emsp;&emsp;同样地，我们可以定义数理统计中的协方差矩阵概念，但这里的协方差矩阵并不是描述两个总体之间相关性，而是用来描述样本各维度之间的相关性。  
&emsp;&emsp;比如我们有一个$m$维的总体  

&emsp;&emsp;$X=(X_1,X_2,\cdots,X_m)^{\mathrm{T} }$，  

有样本$\{(X^{(1)}_1,X^{(1)}_2,\cdots,X^{(1)}_m)^{\mathrm{T} },(X^{(2)}_1,X^{(2)}_2\cdots,X^{(2)}_m)^{\mathrm{T} },\cdots,(X^{(n)}_1,X^{(n)}_2\cdots,X^{(n)}_m)^{\mathrm{T} }\}$， 
 
观察值分别是$\{(x^{(1)}_1,x^{(1)}_2\cdots,x^{(1)}_m)^{\mathrm{T} },(x^{(2)}_1,x^{(2)}_2\cdots,x^{(2)}_m)^{\mathrm{T} },\cdots,(x^{(n)}_1,x^{(n)}_2\cdots,x^{(n)}_m)^{\mathrm{T} }\}$，  

我们想研究这些样本各个维度之间的相关性，可以这样定义样本协方差矩阵：  

记$b_{ij}=Cov(X_i,X_j)=\frac{1}{n-1}\sum_{k=1}^{n}(X^{(k)}_i-\bar{X_i}^{(k)})(X^{(k)}_j-\bar{X_j}^{(k)})$  

则矩阵
$B=\begin{bmatrix}b_{11}&b_{12}&\cdots&b_{1n}\\b_{21}&b_{22}&\cdots&b_{2n}\\\vdots&\vdots& &\vdots\\b_{n1}&b_{n2}&\cdots&b_{nn}\end{bmatrix}$  

为$X$的协方差矩阵

\*\*注意\*\*在计算样本协方差矩阵时，要牢记它是计算同一个样本不同维度之间的协方差，而不是计算不同样本之间的协方差，切记！  

参考资料：
1. 武大版《概率论与数理统计》，齐民友主编。
2. [浅谈协方差矩阵][1]


  [1]: http://pinkyjie.com/2010/08/31/covariance/