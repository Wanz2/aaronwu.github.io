---
title: 深度学习笔记：稀疏自编码器（3）——稀疏自编码算法
date: 2017-04-29 23:43:53
mathjax: true
tags:
	- 机器学习
categories: 
	- 机器学习
description: 有了神经网络和反向传播的基础，我们就可以将其应用到稀疏自编码器中了。
---
&emsp;&emsp;有了神经网络和反向传播的基础，我们就可以将其应用到稀疏自编码器中了。稀疏自编码器属于非监督学习，主要通过尝试学习一个$h_{W,b}(x)\approx x$的函数，来提取输入值$x$中的特征。

<!--more-->
## 0.本文中使用的符号
&emsp;&emsp;本文中使用的符号大体与上一篇文章相同，在此仅列出一些新增加的符号和需要注意的符号。   

|符号|描述|
|---|---|
|$m$|样本总数|
|$a_{j}^{(2)}$|第2层第$j$个神经元的激活度|
|$a_{j}^{(2)}(x)$|在给定输入值为$x$的情况下，第2层第$j$个神经元的激活度|
|$\hat{\rho_{j}}$|$\hat{\rho_{j}}=\frac{1}{m}\sum_{i=1}^{m}[a_{j}^{(2)}(x^{(i)})]$表示第2层第$j$个隐藏神经元在训练集上的平均活跃度|
|$\rho$|表示**稀疏性参数**，通常是一个接近0的值（如$\rho=0.05$），可以令$\hat{\rho_{j}}=\rho$，来对神经元$a_{j}^{(2)}$的稀疏度进行限制|
|$s_2$|第2层（隐藏层）神经元的数量|
|$h_{W,b}(x)$|输入值为$x$，神经网络中权值和偏置项分别为$W,b$的情况下的输出值|

## 1.什么是稀疏自编码器
&emsp;&emsp;先上图：  

![有隐藏层的稀疏自编码器][1]  

&emsp;&emsp;上图为有一个隐藏层的稀疏自编码器示意图。稀疏自编码器为非监督学习，其所使用的样本集  

&emsp;&emsp;$\{x^{(1)},x^{(2)},...,x^{(m)}\}$  

为没有类别标记的样本，我们希望令输出值$h_{W,b}(x)$与输入值$x$近似相等。  

## 2.为什么要用稀疏自编码器
&emsp;&emsp;由于为数据人工增加类别标记是一个非常麻烦的过程，我们希望机器能够自己学习到样本中的一些重要特征。通过对隐藏层施加一些限制，能够使得它在恶劣的环境下学习到能最好表达样本的特征，并能有效地对样本进行降维。这种限制可以是对隐藏层稀疏性的限制。  

## 3.稀疏性限制
### 3.1稀疏性
&emsp;&emsp;当使用sigmoid函数作为激活函数时，若神经元输出值为1，则可认为其被激活，若神经元输出值为0，则可认为其被抑制（使用tanh函数时，代表激活和抑制的值分别为1和-1）。稀疏性限制就是要保证大多数神经元输出为0，即被抑制的状态。  

### 3.2如何限制隐藏层稀疏性
&emsp;&emsp;在本文开始所给出的稀疏自编码网络中，为了限制隐藏结点稀疏性，可以进行如下表示：  
1. 使用$a_{j}^{(2)}$表示第2层第$j$个神经元的激活度。  

2. 使用$a_{j}^{(2)}(x)$表示在给定输入值为$x$的情况下，第2层第$j$个神经元的激活度。  

3. 使用  
&emsp;&emsp;$\hat{\rho_{j}}=\frac{1}{m}\sum_{i=1}^{m}[a_{j}^{(2)}(x^{(i)})]$  
表示第2层第$j$个隐藏神经元在训练集上的平均活跃度。 
 
4. 使用$\rho$表示**稀疏性参数**，通常是一个接近0的值（如$\rho=0.05$），可以令$\hat{\rho_{j}}=\rho$，来对神经元$a_{j}^{(2)}$的稀疏度进行限制。  
  
&emsp;&emsp;我们希望$\hat{\rho_{j}}$和$\rho$越接近越好，因此我们要对这两者有显著差异的情况进行惩罚，惩罚使用KL散度。   
***
**PS. 什么是KL散度？**    
&emsp;&emsp;KL散度又称相对熵，是对两个概率分布P和Q差异的非对称性度量，非对称性意味着$D(P\|Q)\neq D(Q\|P)$，$D(P\|Q)$表示用概率分布$Q$来拟合概率分布$P$时所产生的信息损耗。其定义为：  
  
给定随机变量$s$，  

 若为离散型随机变量：  

&emsp;&emsp;$D(P\|Q)=\sum(p(i)log(\frac{p(i)}{q(i)}))$  

此处p和q表示随机变量的分布律，$p(i)$表示随机变量$s$取$i$的概率。   

若为连续型随机变量：  

&emsp;&emsp;$D(P\|Q)=\int p(s)log(\frac{p(s)}{q(s)})d(s)$  

此处$p$和$q$表示随机变量$s$的概率密度。  

&emsp;&emsp;KL散度的性质是，当$P=Q$时值为0，随着$P$和$Q$差异增大而递增。  
***
&emsp;&emsp;在这里，我们是要用$\hat{\rho_j}$去逼近$\rho$，这里的KL散度是：  

&emsp;&emsp;$\sum_{j=1}^{s_2}KL(\rho \| \hat{\rho}_j)=\sum_{j=1}^{s_2}\rho log\frac{\rho}{\hat{\rho}_j}+(1-\rho)log\frac{1-\rho}{1-\hat{\rho}_j}$  

&emsp;&emsp;于是我们在代价函数中加入这一惩罚因子，代价函数就变成：  

&emsp;&emsp;$J_{sparse}(W,b)=J(W,b)+\beta \sum_{j=1}^{s_2}KL(\rho \| \hat{\rho}_j)$   

代价函数改变了，在反向传导时残差公式也要做出相应的改变，之前隐藏层第$i$个结点残差为：  

&emsp;&emsp;$\delta_{i}^{(2)}=(\sum_{j=1}^{s_{3}}W_{ji}^{(2)}\delta_{j}^{(3)})f'(z_{i}^{(2)})$  

现在应该将其换成：  

&emsp;&emsp;$\delta_{i}^{(2)}=(\sum_{j=1}^{s_{3}}W_{ji}^{(2)}\delta_{j}^{(3)}+\beta (-\frac{\rho}{\hat{\rho}_i}+\frac{1-\rho}{1-\hat{\rho}_i}))f'(z_{i}^{(2)})$  

***  

注意：在ufldl的[自编码算法和稀疏性][2]中的后向传播算法里，提到隐藏层第$i$个结点残差为：  

![隐藏层结点残差][3]  

但根据ufldl教程[反向传导算法][4]一节的推导，残差递推公式为：  

![残差递推公式][5]  

笔者自己根据公式推了一遍，同时加上自己的理解，笔者认为在  

![隐藏层结点残差][6]  

中，累加的上限应该是$s_3$而不是$s_2$，因此在本文最后处的公式里写的是$s_3$，但笔者由于还是在校学生，知识有限，此处还是存在疑问，恳请看到本文的同学能在这里指点一二，感激不尽！


  [1]: http://odnk9as2f.bkt.clouddn.com/400px-Autoencoder636.png
  [2]: http://deeplearning.stanford.edu/wiki/index.php/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7
  [3]: http://odnk9as2f.bkt.clouddn.com/%E9%9A%90%E5%B1%82%E6%AE%8B%E5%B7%AE%E5%85%AC%E5%BC%8F
  [4]: http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95
  [5]: http://odnk9as2f.bkt.clouddn.com/%E6%AE%8B%E5%B7%AE%E9%80%92%E6%8E%A8%E5%85%AC%E5%BC%8F
  [6]: http://odnk9as2f.bkt.clouddn.com/%E9%9A%90%E5%B1%82%E6%AE%8B%E5%B7%AE%E5%85%AC%E5%BC%8F