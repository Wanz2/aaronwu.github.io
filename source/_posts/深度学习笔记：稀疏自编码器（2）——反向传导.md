---
title: 深度学习笔记：稀疏自编码器（2）——反向传导
date: 2017-04-29 23:41:01
mathjax: true
tags:
	- 机器学习
categories: 
	- 机器学习
description: 本文是深度学习笔记的第二篇，上一篇文章《神经元与神经网络》中讲到了前向传播算法，本文中将介绍如何进行参数的优化，即用反向传导。  
---
&emsp;&emsp;本文是深度学习笔记的第二篇，上一篇文章[《神经元与神经网络》][1]中讲到了前向传播算法，本文中将介绍如何进行参数的优化，即用反向传导。   

<!-- more -->
## 0.本文中所使用的符号和一些约定
&emsp;&emsp;本文中所使用的样本集：  

$\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$  

&emsp;&emsp;其他符号：  
  
|符号|描述|
|---|---|
|$(x^{(j)},y^{(j)})$|第$j$个样本|
|$m$|样本总数|
|$(x,y)$|单个样本|
|$x_i$|第$i$个输入值|
|$f()$|神经元激活函数，本文中为sigmoid函数，即$f(z)=\frac{1}{1+e^{(-z)}}$|
|$W_{ij}^{(l)}$|神经网络中第$l$层第$j$个神经元和第$l+1$层第$i$个神经元连线上的权值|
|$b_i^{(l)}$|第$l+1$层第$i$个神经元输入的偏置项|
|$n_l$|神经网络的总层数|
|$L_l$|第$l$层，则输入层为$L_1$，输出层为$L_{nl}$|
|$s_l$|第$l$层的节点数（不包括偏置单元）|
|$a_i^{l}$|第$l$层第$i$单元的激活值（输出值），当$l=1$时，$a_i^{(1)}=x_i$|
|$z_i^{(l)}$|第$l$层第$i$单元输入加权和（包括偏置单元），有$a_i^{(l)}=f(z_i^{(l)})$|
|$h_{W,b}(x)$|输入值为$x$，神经网络中权值和偏置项分别为$W,b$的情况下的输出值|  

&emsp;&emsp;**约定** 本文中将函数$f()$以及偏导数$f'()$进行了针对向量参数的扩展，即：  

&emsp;&emsp;$f([z_1,z_2,z_3])=[f(z_1),f(z_2),f(z_3)]$  
  
&emsp;&emsp;$f'([z_1,z_2,z_3])=[f'(z_1),f'(z_2),f'(z_3)]$  

## 1.代价函数
&emsp;&emsp;要求解神经网络，就要通过最优化神经网络的代价函数(cost function)而得出其中的参数$W$和$b$的值。     
&emsp;&emsp;对于单个样例$(x,y)$下，代价函数为：  
 
&emsp;&emsp;$J(W,b;x,y)=\frac{1}{2}\|h_{w,b}(x)-y\|^{2}$  
 
&emsp;&emsp;对于一个包含$m$个样例的数据集，整体代价函数为：  

$J(W,b)
=[\frac{1}{m}\sum_{i=1}^{m}J(W,b;x^{(i)},y^{(i)})]+\frac{\lambda}{2}\sum_{l=1}^{n_{l}-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^{2}$  
 
&emsp;&emsp;&emsp;$=[\frac{1}{m}\sum_{i=1}^{m}(\frac{1}{2}\|h_{w,b}(x^{(i)})-y^{(i)}\|^{2})]+\frac{\lambda}{2}\sum_{l=1}^{n_{l}-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}(W_{ji}^{(l)})^{2}$  

&emsp;&emsp;其中第二项是一个规则化项（也叫权重衰减项），其目的是减小权重的幅度，防止过拟合，而$\lambda$则用来控制第一项和第二项的相对重要性  

## 2.梯度下降
&emsp;&emsp;为了优化代价函数，要进行以下几步：  
  
1. 初始化每一个参数$W_{ij}^{(l)}$和$b_i^{(l)}$为很小的接近零的随机值  
2. 使用最优化算法，诸如批量梯度下降法，梯度下降公式如下：  

&emsp;&emsp;$W_{ij}^{(l)}=W_{ij}^{(l)}-\alpha\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b)$  

&emsp;&emsp;$b_{i}^{(l)}=b_{i}^{(l)}-\alpha\frac{\partial}{\partial b_{i}^{(l)}}J(W,b)$   

其中$\alpha$是学习速率。    

&emsp;&emsp;**计算梯度下降的关键步骤是计算偏导数，此时反向传导算法就要登场了。**其实反向传导算法的思想和高数里复合函数求导的思想是一样的。  
## 3.反向传导
&emsp;&emsp;我们将求偏导的项单独拿出来看：  
  
&emsp;&emsp;$\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b)=[\frac{1}{m}\sum_{i=1}^{m}\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x^{(i)},y^{(i)})]+\lambda W_{ij}^{(l)}$&emsp;&emsp;&emsp;(1)  

&emsp;&emsp;$\frac{\partial}{\partial b_{i}^{(l)}}J(W,b)$=$\frac{1}{m}\sum_{i=1}^{m}\frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x^{(i)},y^{(i)})$&emsp;&emsp;&emsp;(2)  

显然，求出$\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x,y)$和$\frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x,y)$即可求出$\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b)$和$\frac{\partial}{\partial b_{i}^{(l)}}J(W,b)$。  

由于：  

&emsp;&emsp;$\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x,y)=\frac{\partial J(W,b;x,y)}{\partial z_{i}^{(l+1)}}\frac{\partial z_{i}^{l+1}}{\partial W_{ij}^{l}}$&emsp;&emsp;&emsp;(3)   

&emsp;&emsp;$\frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x,y)=\frac{\partial J(W,b;x,y)}{\partial z_{i}^{(l+1)}}\frac{\partial z_{i}^{(l+1)}}{\partial b_{i}^{(l)}}$&emsp;&emsp;&emsp;(4)  

因此要先求$\frac{\partial J(W,b;x,y)}{\partial z_{i}^{(l)}}$,在UFLDL中，将之称为“残差”，用$\delta_{i}^{(l)}$表示，该残差表示第$l$层的第$i$个结点对最终输出值的残差产生了多少影响。  
 
&emsp;&emsp;计算$\frac{\partial J(W,b;x,y)}{\partial z_{i}^{(l)}}$可得：  

&emsp;&emsp;$\delta_{i}^{(n_l)}=-(y_{i}-a_{i}^{(n_l)})f'(z_{i}^{(n_l)})$  

&emsp;&emsp;$\delta_{i}^{(l)}=(\sum_{j=1}^{s_{l+1}}W_{ji}^{(l)}\delta_{j}^{(l+1)})f'(z_{i}^{(l)})$  
 
通过以上第一式可以求出第$n_l$层结点的残差，而第二式则表达了第$l$层结点残差与第$l+1$层结点残差的关系，通过将两式回代进式(3)和式(4)，则可以得到：  
  
&emsp;&emsp;$\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x,y)=\delta _{i}^{(l+1)}a_{j}^{(l)}$&emsp;&emsp;&emsp;(5)  
  
&emsp;&emsp;$\frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x,y)=\delta_{i}^{(l+1)}$&emsp;&emsp;&emsp;(6)    

再将上面两式代入(1)和(2)就能够求出整体的偏导数了。   


  [1]: http://blog.csdn.net/wanz2/article/details/52926736
  [2]: http://odnk9as2f.bkt.clouddn.com/2016-11-01%20001059.jpg?imageView/2/w/600/q/90