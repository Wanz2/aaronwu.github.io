---
title: 深度学习笔记：稀疏自编码器（1）——神经元与神经网络
date: 2017-04-29 23:38:52
mathjax: true
tags: 
	- 机器学习
categories: 
	- 机器学习
description: 笔者在很久以前就已经学习过UFLDL深度学习教程中的稀疏自编码器，近期需要用到的时候发现有些遗忘，温习了一遍之后决定在这里做一下笔记。
---
&emsp;&emsp;笔者在很久以前就已经学习过UFLDL深度学习教程中的稀疏自编码器，近期需要用到的时候发现有些遗忘，温习了一遍之后决定在这里做一下笔记，本文不是对神经元与神经网络的介绍，而是笔者学习之后做的归纳和整理，打算分为几篇记录。详细教程请见[UFLDL教程][1]，看完教程之后再来看本文可能会更加清晰。  

<!-- more -->
## 0. 本文中使用的符号一览及本文中的一些约定
|符号|描述|
|---|---|
|$x$|输入值向量，为$n$维|
|$x_i$|第$i$个输入值|
|$f()$|神经元激活函数，本文中为sigmoid函数，即$f(z)=\frac{1}{1+e^{(-z)}}$|
|$W_{ij}^{(l)}$|神经网络中第$l$层第$j$个神经元和第$l+1$层第$i$个神经元连线上的权值|
|$b_i^{(l)}$|第$l+1$层第$i$个神经元输入的偏置项|
|$n_l$|神经网络的总层数|
|$L_l$|第$l$层，则输入层为$L_1$，输出层为$L_{nl}$|
|$s_l$|第$l$层的节点数（不包括偏置单元）|
|$a_i^{l}$|第$l$层第$i$单元的激活值（输出值），当$l=1$时，$a_i^{(1)}=x_i$|
|$z_i^{(l)}$|第$l$层第$i$单元输入加权和（包括偏置单元），有$a_i^{(l)}=f(z_i^{(l)})$|
|$h_{W,b}(x)$|输入值为$x$，神经网络中权值和偏置项分别为$W,b$的情况下的输出值|  

&emsp;&emsp;**约定** 本文中将函数$f()$以及偏导数$f'()$进行了针对向量参数的扩展，即：  

&emsp;&emsp;$f([z_1,z_2,z_3])=[f(z_1),f(z_2),f(z_3)]$  
 
&emsp;&emsp;$f'([z_1,z_2,z_3])=[f'(z_1),f'(z_2),f'(z_3)]$  

## 1. 神经元
### 1.1 神经元定义
&emsp;&emsp;神经元是以$x$为输入，$h_{W,b}$为输出的运算单元，如图所示：      

![神经元][2]  

### 1.2 激活函数
&emsp;&emsp;通常在神经元中输出值为0表示神经元被激活，输出为1表示神经元被抑制，而计算输出值所用的函数则被称为“激活函数”。本文中所用的激活函数为sigmoid函数：  

&emsp;&emsp;&emsp;&emsp;$f(z)=\frac{1}{1+e^{(-z)}}$，其定义域为$(-\infty,+\infty)$，值域为$(0,1)$。  

&emsp;&emsp;以上图神经元为例，假设第$i$个输入值与神经元连线上的权重为$w_1$，输入值偏置项为$b_1$，在单个神经元中，有： 
      
&emsp;&emsp;$h_{W,b}(x)=f(x_1w_1+x_2w_2+x_3w_3+b_1)$   

&emsp;&emsp;观察可以发现，单个神经元就是一个**逻辑回归**模型。  
## 2. 神经网络
### 2.1 神经网络定义
&emsp;&emsp;神经网络就是将多个神经元连在一起，前一层神经元的输出就是后一层神经元的输入，下图是一个三层神经网络：  

![三层神经网络模型][3]  

&emsp;&emsp;以上图为例，最左边一层为**输入层**，中间一层为**隐藏层**，最右边一层为**输出层**。  
### 2.2 前向传播
&emsp;&emsp;前向传播就是使用输入值$x$通过神经网络计算出输出值$h_{W,b}(x)$的过程。以上图神经网络为例，前向传播中每一个神经元输入输出过程如下：  
  
&emsp;&emsp;令$z_i^{(l)}$表示第$l$层第$i$个神经元的输入值，有  
 
&emsp;&emsp;$z_i^{(l+1)}=W_{i1}^{(l)}x_1+W_{i2}^{(l)}x_2+W_{i3}^{(l)}x_3+b_i^{(l)}$  （这里其实是一个**线性回归**模型）  
  
&emsp;&emsp;$a_i^{(l)}=f(z_i^{(l)})$  

&emsp;&emsp;$h_{W,b}(x)=a_1^{(3)}$  

### 2.3 前向传播的矩阵表示
&emsp;&emsp;本文中扩展了函数$f()$针对向量参数的约定，即有： 
  
&emsp;&emsp;$f([z_1,z_2,z_3])=[f(z_1),f(z_2),f(z_3)]$  

则上图神经网络中的前向传播过程可表示为： 
 
&emsp;&emsp;$z^{(2)}=W^{(1)}x+b^{(1)}$  

&emsp;&emsp;$a^{(2)}=f(z^{(2)})$  

&emsp;&emsp;$z^{(3)}=W^{(2)}a^{(2)}+b^{(2)}$  

&emsp;&emsp;$h_{W,b}(x)=f(z^{(3)})$  

下面以 $z^{(2)}=W^{(1)}x+b^{(1)}$ 为例，展开一下具体的矩阵表示，其他式子略：  
&emsp;&emsp;
$$
\begin{pmatrix}
z_1^{(2)}\\
z_2^{(2)}\\
z_3^{(2)}\\
\end{pmatrix}=
\begin{pmatrix}
W_{11}^{(1)}&W_{12}^{(1)}&W_{13}^{(1)}\\
W_{21}^{(1)}&W_{22}^{(1)}&W_{23}^{(1)}\\
W_{31}^{(1)}&W_{32}^{(1)}&W_{33}^{(1)}\\
\end{pmatrix}
\begin{pmatrix}x_1\\
x_2\\
x_3\\
\end{pmatrix}+
\begin{pmatrix}
b_1^{(1)}\\
b_2^{(1)}\\
b_3^{(1)}\\
\end{pmatrix}
$$  

最终，神经网络前向传播过程可表示为：  

&emsp;&emsp;$z^{(l+1)}=W^{(l)}a^{(l)}+b^{(l)}$  

&emsp;&emsp;$a^{(l+1)}=f(z^{(l+1)})$   

  [1]: http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B
  [2]: http://odnk9as2f.bkt.clouddn.com/300px-SingleNeuron.png
  [3]: http://odnk9as2f.bkt.clouddn.com/400px-Network331.png