<!DOCTYPE html>
<html lang="undefined">



<!-- Head tag -->
<head>
    
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no">
    
    <title>
    
        深度学习笔记：稀疏自编码器（2）——反向传导 - 
    
    Aaron Wu
    </title>
    
    <meta name="keywords" content="">
    <meta name="description" content="Aaron Wu">
    <meta name="author" content="Aaron Wu">
    
    
    
    
    
    <link href="/css/style.css" rel="stylesheet" type="text/css">
    <link href="https://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://cdn.bootcss.com/cc-icons/1.2.1/css/cc-icons.min.css" rel="stylesheet" type="text/css">
    <script src="https://cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>
    <!--<script src="https://cdn.bootcss.com/jquery_lazyload/1.9.7/jquery.lazyload.min.js"></script>-->
    
    
    <script src="/js/script.js" type="text/javascript" async></script>

    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        showProcessingMessages: false,
        jax: ["input/TeX","output/HTML-CSS", "output/PreviewHTML"], 
        extensions: ["tex2jax.js", "fast-preview.js", "AssistiveMML.js" /*"MathMenu.js", "MathZoom.js", "[Contrib]/a11y/accessibility-menu.js"*/ ],
        TeX: {
            extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
        }, 
        tex2jax: {
            inlineMath: [['$','$']],
            displayMath: [['$$','$$']], 
            processEscapes: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        },
        "HTML-CSS": {linebreaks: {automatic: true}},
        SVG: {linebreaks: {automatic: true}}
    });
    </script>
    <script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js" async></script>
    

    
    <script type="text/javascript">
    var duoshuo_admin_id = "6224035036874670849";
    var duoshuoQuery = {short_name: 'unnamed42-github' };
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = '//static.duoshuo.com/embed.js';
        ds.id = "duoshuo-script";
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
    </script>
    
    <script src="/js/duoshuo-hook.js" async></script>
    
    

    
    
    

    
    <script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

</head>


<body class="post">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="header">
    <div class="container clearfix">
        <div class="logo" style="width: auto;">
            <a href="/" title="Aaron Wu" rel="home" style="text-decoration: none;">
                
                <span class="brand-logo">
                    
                    Aaron Wu
                    
                
                </span>
            </a>
        </div>
        <div id="global-nav">
            <ul class="gnul">
                <li class="gnli ">
                    <a class="gna fa fa-home" href="/" title="Aaron Wu">
                        <span class="gn-item">主页</span>
                    </a>
                </li>
                
                <li class="gnli dropdown current">
                    <a class="gna fa fa-bars" href="#">
                        <span class="gn-item">导航</span>
                    </a>
                    <div class="submenu">
                        <div class="tab-content">
                            <table>
                                <tbody>
                                    
                                    <tr class="trline">
                                        <td class="tdleft">分类</td>
                                        <td class="tdright">
                                            <ul class="tab-categories">
                                            
                                                <li class="cat-item">
                                                    <a href="/categories/Java/">Java</a>
                                                </li>
                                            
                                                <li class="cat-item">
                                                    <a href="/categories/Linux/">Linux</a>
                                                </li>
                                            
                                                <li class="cat-item">
                                                    <a href="/categories/图像处理/">图像处理</a>
                                                </li>
                                            
                                                <li class="cat-item">
                                                    <a href="/categories/机器学习/">机器学习</a>
                                                </li>
                                            
                                            </ul>
                                        </td>
                                    </tr>
                                    
                                    <tr>
                                        <td class="tdleft">标签</td>
                                        <td class="tdright">
                                            <div class="tab-tags">
                                                <a href="/tags/Java/" style="font-size: 12px;">Java</a> <a href="/tags/机器学习/" style="font-size: 12px;">机器学习</a> <a href="/tags/Linux/" style="font-size: 12px;">Linux</a> <a href="/tags/图像处理/" style="font-size: 12px;">图像处理</a>
                                            </div>
                                        </td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </li>
                
                
                <li class="gnli">
                    <a class="gna fa fa-archive" href="/archives/index.html" title="">
                        <span class="gn-item">归档</span>
                    </a>
                </li>
                
                <li class="gnli">
                    <a class="gna fa fa-user" href="/about/index.html" title="">
                        <span class="gn-item">关于</span>
                    </a>
                </li>
                
                <li class="gnli">
                    <a class="gna fa fa-rss" href="/atom.xml" title="">
                        <span class="gn-item">订阅</span>
                    </a>
                </li>
                
                
                
            </ul>
        </div>
        
<form method="get" id="search-form" action="/search.html">
    <input type="text" name="keywords" id="search-input" placeholder="输入关键词" />
    <button type="submit" id="search-submit">
        <span class="fa fa-search"></span>
    </button>
</form>


    </div>
</nav>

    
<main id="content">
    <div class="container clearfix">
        <div id="primary" >
            <nav class="breadcrumb-navigation">
                <a rel="bookmark" href="/">主页</a>
                <span class="breadcrumb-arrow fa fa-angle-right"></span>
                 
<a href="/categories/机器学习/" title="机器学习" rel="category tag">机器学习</a>

                <span class="breadcrumb-arrow fa fa-angle-right"></span>
                深度学习笔记：稀疏自编码器（2）——反向传导
            </nav>
            <article class="single-post">
                <header class="post-header">
                    <h1 class="post-title">
                        <a href="/2017/04/29/深度学习笔记：稀疏自编码器（2）——反向传导/" title="深度学习笔记：稀疏自编码器（2）——反向传导">深度学习笔记：稀疏自编码器（2）——反向传导</a>
                    </h1>
                </header>
                <div class="post-meta">
    <ul class="inline-ul">
        <li class="inline-li">
            <time>2017-04-29</time>
        </li>
        
        <li class="inline-li">
            <span class="post-span">·</span>
        </li>
        <li class="inline-li">
             
<a href="/categories/机器学习/" title="机器学习" rel="category tag">机器学习</a>

        </li>
        
        
        <li class="inline-li">
            <span class="post-span">·</span>
        </li>
        <li class="inline-li">
            <span id="busuanzi_value_page_pv"></span> Views
        </li>
        
    </ul>
</div> 

                <div class="post-body clearfix">
                    <div class="post-content">
                        
                        <p>&emsp;&emsp;本文是深度学习笔记的第二篇，上一篇文章<a href="http://blog.csdn.net/wanz2/article/details/52926736" target="_blank" rel="external">《神经元与神经网络》</a>中讲到了前向传播算法，本文中将介绍如何进行参数的优化，即用反向传导。   </p>
<a id="more"></a>
<h2 id="0-本文中所使用的符号和一些约定"><a href="#0-本文中所使用的符号和一些约定" class="headerlink" title="0.本文中所使用的符号和一些约定"></a>0.本文中所使用的符号和一些约定</h2><p>&emsp;&emsp;本文中所使用的样本集：<br>${(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})}$<br>&emsp;&emsp;其他符号：  </p>
<table>
<thead>
<tr>
<th>符号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>$(x^{(j)},y^{(j)})$</td>
<td>第$j$个样本</td>
</tr>
<tr>
<td>$m$</td>
<td>样本总数</td>
</tr>
<tr>
<td>$(x,y)$</td>
<td>单个样本</td>
</tr>
<tr>
<td>$x_i$</td>
<td>第$i$个输入值</td>
</tr>
<tr>
<td>$f()$</td>
<td>神经元激活函数，本文中为sigmoid函数，即$f(z)=\frac{1}{1+e^{(-z)}}$</td>
</tr>
<tr>
<td>$W_{ij}^{(l)}$</td>
<td>神经网络中第$l$层第$j$个神经元和第$l+1$层第$i$个神经元连线上的权值</td>
</tr>
<tr>
<td>$b_i^{(l)}$</td>
<td>第$l+1$层第$i$个神经元输入的偏置项</td>
</tr>
<tr>
<td>$n_l$</td>
<td>神经网络的总层数</td>
</tr>
<tr>
<td>$L_l$</td>
<td>第$l$层，则输入层为$L<em>1$，输出层为$L</em>{nl}$</td>
</tr>
<tr>
<td>$s_l$</td>
<td>第$l$层的节点数（不包括偏置单元）</td>
</tr>
<tr>
<td>$a_i^{l}$</td>
<td>第$l$层第$i$单元的激活值（输出值），当$l=1$时，$a_i^{(1)}=x_i$</td>
</tr>
<tr>
<td>$z_i^{(l)}$</td>
<td>第$l$层第$i$单元输入加权和（包括偏置单元），有$a_i^{(l)}=f(z_i^{(l)})$</td>
</tr>
<tr>
<td>$h_{W,b}(x)$</td>
<td>输入值为$x$，神经网络中权值和偏置项分别为$W,b$的情况下的输出值</td>
</tr>
</tbody>
</table>
<p>&emsp;&emsp;<strong>约定</strong> 本文中将函数$f()$以及偏导数$f’()$进行了针对向量参数的扩展，即：<br>&emsp;&emsp;$f([z_1,z_2,z_3])=[f(z_1),f(z_2),f(z_3)]$<br>&emsp;&emsp;$f’([z_1,z_2,z_3])=[f’(z_1),f’(z_2),f’(z_3)]$  </p>
<h2 id="1-代价函数"><a href="#1-代价函数" class="headerlink" title="1.代价函数"></a>1.代价函数</h2><p>&emsp;&emsp;要求解神经网络，就要通过最优化神经网络的代价函数(cost function)而得出其中的参数$W$和$b$的值。<br>&emsp;&emsp;对于单个样例$(x,y)$下，代价函数为：<br>&emsp;&emsp;$J(W,b;x,y)=\frac{1}{2}|h<em>{w,b}(x)-y|^{2}$<br>&emsp;&emsp;对于一个包含$m$个样例的数据集，整体代价函数为：<br>$J(W,b)<br>=[\frac{1}{m}\sum</em>{i=1}^{m}J(W,b;x^{(i)},y^{(i)})]+\frac{\lambda}{2}\sum<em>{l=1}^{n</em>{l}-1}\sum<em>{i=1}^{s</em>{l}}\sum<em>{j=1}^{s</em>{l+1}}(W<em>{ji}^{(l)})^{2}$<br>&emsp;&emsp;&emsp;$=[\frac{1}{m}\sum</em>{i=1}^{m}(\frac{1}{2}|h<em>{w,b}(x^{(i)})-y^{(i)}|^{2})]+\frac{\lambda}{2}\sum</em>{l=1}^{n<em>{l}-1}\sum</em>{i=1}^{s<em>{l}}\sum</em>{j=1}^{s<em>{l+1}}(W</em>{ji}^{(l)})^{2}$<br>&emsp;&emsp;其中第二项是一个规则化项（也叫权重衰减项），其目的是减小权重的幅度，防止过拟合，而$\lambda$则用来控制第一项和第二项的相对重要性  </p>
<h2 id="2-梯度下降"><a href="#2-梯度下降" class="headerlink" title="2.梯度下降"></a>2.梯度下降</h2><p>&emsp;&emsp;为了优化代价函数，要进行以下几步：    </p>
<ol>
<li>初始化每一个参数$W_{ij}^{(l)}$和$b_i^{(l)}$为很小的接近零的随机值  </li>
<li>使用最优化算法，诸如批量梯度下降法，梯度下降公式如下：<br>&emsp;&emsp;$W<em>{ij}^{(l)}=W</em>{ij}^{(l)}-\alpha\frac{\partial}{\partial W<em>{ij}^{(l)}}J(W,b)$<br>&emsp;&emsp;$b</em>{i}^{(l)}=b<em>{i}^{(l)}-\alpha\frac{\partial}{\partial b</em>{i}^{(l)}}J(W,b)$<br>其中$\alpha$是学习速率。<br>&emsp;&emsp;<strong>计算梯度下降的关键步骤是计算偏导数，此时反向传导算法就要登场了。</strong>其实反向传导算法的思想和高数里复合函数求导的思想是一样的。  <h2 id="3-反向传导"><a href="#3-反向传导" class="headerlink" title="3.反向传导"></a>3.反向传导</h2>&emsp;&emsp;我们将求偏导的项单独拿出来看：<br>&emsp;&emsp;$\frac{\partial}{\partial W<em>{ij}^{(l)}}J(W,b)=[\frac{1}{m}\sum</em>{i=1}^{m}\frac{\partial}{\partial W<em>{ij}^{(l)}}J(W,b;x^{(i)},y^{(i)})]+\lambda W</em>{ij}^{(l)}$&emsp;&emsp;&emsp;(1)<br>&emsp;&emsp;$\frac{\partial}{\partial b<em>{i}^{(l)}}J(W,b)$=$\frac{1}{m}\sum</em>{i=1}^{m}\frac{\partial}{\partial b<em>{i}^{(l)}}J(W,b;x^{(i)},y^{(i)})$&emsp;&emsp;&emsp;(2)<br>&emsp;&emsp;显然，求出$\frac{\partial}{\partial W</em>{ij}^{(l)}}J(W,b;x,y)$和$\frac{\partial}{\partial b<em>{i}^{(l)}}J(W,b;x,y)$即可求出$\frac{\partial}{\partial W</em>{ij}^{(l)}}J(W,b)$和$\frac{\partial}{\partial b<em>{i}^{(l)}}J(W,b)$。<br>&emsp;&emsp;由于：<br>&emsp;&emsp;$\frac{\partial}{\partial W</em>{ij}^{(l)}}J(W,b;x,y)=\frac{\partial J(W,b;x,y)}{\partial z<em>{i}^{(l+1)}}\frac{\partial z</em>{i}^{l+1}}{\partial W<em>{ij}^{l}}$&emsp;&emsp;&emsp;(3)<br>&emsp;&emsp;$\frac{\partial}{\partial b</em>{i}^{(l)}}J(W,b;x,y)=\frac{\partial J(W,b;x,y)}{\partial z<em>{i}^{(l+1)}}\frac{\partial z</em>{i}^{(l+1)}}{\partial b<em>{i}^{(l)}}$&emsp;&emsp;&emsp;(4)<br>因此要先求$\frac{\partial J(W,b;x,y)}{\partial z</em>{i}^{(l)}}$,在UFLDL中，将之称为“残差”，用$\delta<em>{i}^{(l)}$表示，该残差表示第$l$层的第$i$个结点对最终输出值的残差产生了多少影响。<br>&emsp;&emsp;计算$\frac{\partial J(W,b;x,y)}{\partial z</em>{i}^{(l)}}$可得：<br>&emsp;&emsp;$\delta_{i}^{(n<em>l)}=-(y</em>{i}-a_{i}^{(n<em>l)})f’(z</em>{i}^{(n<em>l)})$<br>&emsp;&emsp;$\delta</em>{i}^{(l)}=(\sum<em>{j=1}^{s</em>{l+1}}W<em>{ji}^{(l)}\delta</em>{j}^{(l+1)})f’(z_{i}^{(l)})$<br>通过以上第一式可以求出第$n<em>l$层结点的残差，而第二式则表达了第$l$层结点残差与第$l+1$层结点残差的关系，通过将两式回代进式(3)和式(4)，则可以得到：<br>&emsp;&emsp;$\frac{\partial}{\partial W</em>{ij}^{(l)}}J(W,b;x,y)=\delta <em>{i}^{(l+1)}a</em>{j}^{(l)}$&emsp;&emsp;&emsp;(5)<br>&emsp;&emsp;$\frac{\partial}{\partial b<em>{i}^{(l)}}J(W,b;x,y)=\delta</em>{i}^{(l+1)}$&emsp;&emsp;&emsp;(6)<br>再将上面两式代入(1)和(2)就能够求出整体的偏导数了。   </li>
</ol>

                    </div>
                </div>
                <nav class="post-tags">
                
                    
                    <a href="/tags/机器学习/" title="机器学习" rel="tag" class="fa fa-tag">机器学习</a>
                    
                
                </nav>
            </article>
        </div><!-- div#primary -->
        <ul id="pager">

    <li class="previous">
        <a href="/2017/04/29/深度学习笔记：稀疏自编码器（3）——稀疏自编码算法/" title="深度学习笔记：稀疏自编码器（3）——稀疏自编码算法" class="tooltipped tooltipped-n" aria-label="深度学习笔记：稀疏自编码器（3）——稀疏自编码算法"><span class="fa fa-chevron-left"></span> 上一篇文章</a>
    </li>
    
    
    <li class="next">
        <a href="/2017/04/29/深度学习笔记：稀疏自编码器（1）————神经元与神经网络/" title="深度学习笔记：稀疏自编码器（1）——神经元与神经网络" class="tooltipped tooltipped-n" aria-label="深度学习笔记：稀疏自编码器（1）——神经元与神经网络">下一篇文章 <span class="fa fa-chevron-right"></span></a>
    </li>
    
</ul>

        
<section id="comments">
    
    <!-- Duoshuo start -->
    <div class="ds-thread"
        data-thread-key="2017/04/29/深度学习笔记：稀疏自编码器（2）——反向传导/"
        data-title="深度学习笔记：稀疏自编码器（2）——反向传导"
        data-url="https://wanz2.github.io/2017/04/29/深度学习笔记：稀疏自编码器（2）——反向传导/" >
    </div>
    <!-- Duoshuo end -->
    

    
</section>


        
    </div>
</main>

    <!-- Footer -->
    
<!-- #content -->
<footer id="footer">
    <div class="container">
        <p>
            <span class="cc-license">
                <a href="https://creativecommons.org/licenses/by-sa/4.0" target="_blank" title="Creative Commons license by-sa">
                <span class="cc cc-cc"></span>
                
                    <span class="cc cc-by"></span>
                
                    <span class="cc cc-sa"></span>
                
                </a>
            </span>
            &nbsp;2015-2017 Aaron Wu.
        </p>
        <p>Themed by <a href="https://github.com/unnamed42/hexo-theme-kunkka">Kunkka</a> | Powered by <a href="https://hexo.io/">Hexo</a>.</p>
        


<div class="sns-icons">
    
        
        
            
    <span class="sns-icon fa-stack">
        <a target="_blank" href="https://github.com/unnamed42">
            <span class="fa fa-square fa-stack-2x"></span>
            
            <span class="fa fa-github fa-stack-1x fa-inverse"></span>
            
        </a>
    </span>
        
    
        
        
    
        
        
    
        
        
    
        
        
            
    <span class="sns-icon fa-stack">
        <a target="_blank" href="https://plus.google.com/u/0/101538012089625781458">
            <span class="fa fa-square fa-stack-2x"></span>
            
            <span class="fa fa-google-plus fa-stack-1x fa-inverse"></span>
            
        </a>
    </span>
        
    
        
        
    
        
        
    
        
        
            
    <span class="sns-icon fa-stack">
        <a target="_blank" href="https://www.zhihu.com/people/huang-wen-rui-74">
            <span class="fa fa-square fa-stack-2x"></span>
            
            <span class="fa fa-stack-1x fa-inverse">知</span>
            
        </a>
    </span>
        
    
        
        
            
    <span class="sns-icon fa-stack">
        <a target="_blank" href="http://tieba.baidu.com/home/main/?un=Xelnagaman">
            <span class="fa fa-square fa-stack-2x"></span>
            
            <span class="fa fa-stack-1x fa-inverse">贴</span>
            
        </a>
    </span>
        
    
</div>


    </div>
</footer>
<!-- #footer -->

    <div id="totop" title="送你上天">
    <canvas id="totop-canvas" width="48" height="48"></canvas>
    <div id="totop-percent"></div>
</div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>

</html>
