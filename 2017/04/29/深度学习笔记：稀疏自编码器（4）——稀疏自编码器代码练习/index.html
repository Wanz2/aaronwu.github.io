<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习 | Aaron Wu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="机器学习" />
  
  
  
  
  <meta name="description" content="&amp;emsp;&amp;emsp;本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见Exercise:Sparse Autoencoder，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习">
<meta property="og:url" content="https://wanz2.github.io/2017/04/29/深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习/index.html">
<meta property="og:site_name" content="Aaron Wu">
<meta property="og:description" content="&amp;emsp;&amp;emsp;本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见Exercise:Sparse Autoencoder，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。">
<meta property="og:updated_time" content="2017-04-29T15:50:51.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习">
<meta name="twitter:description" content="&amp;emsp;&amp;emsp;本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见Exercise:Sparse Autoencoder，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。">
  
    <link rel="alternate" href="/atom.xml" title="Aaron Wu" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css" >
  <link rel="stylesheet" href="/css/hiero.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >

</head>

<script>
var themeMenus = {};

  themeMenus["/"] = "Home"; 

  themeMenus["/archives"] = "Archives"; 

  themeMenus["/categories"] = "Categories"; 

  themeMenus["/about"] = "About"; 

</script>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="Aaron Wu" rel="home"> Aaron Wu </a>
            
          </h1>

          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">Archives</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">Categories</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">About</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>




  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2017/04/29/深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习/" class="article-date">
	  <time datetime="2017-04-29T15:45:57.000Z" itemprop="datePublished">April 29, 2017</time>
	</a>

      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>&emsp;&emsp;本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见<a href="http://deeplearning.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder" target="_blank" rel="external">Exercise:Sparse Autoencoder</a>，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line">% sampleIMAGES.m</div><div class="line">function patches = sampleIMAGES() % 函数返回64*10000的矩阵patches</div><div class="line">% sampleIMAGES</div><div class="line">% Returns 10000 patches for training</div><div class="line"></div><div class="line">load IMAGES;    % load images from disk </div><div class="line"></div><div class="line">patchsize = 8;  % we&apos;ll use 8x8 patches </div><div class="line">numpatches = 10000;</div><div class="line"></div><div class="line">% Initialize patches with zeros.  Your code will fill in this matrix--one</div><div class="line">% column per patch, 10000 columns. </div><div class="line">patches = zeros(patchsize*patchsize, numpatches);</div><div class="line"></div><div class="line">%% ---------- YOUR CODE HERE --------------------------------------</div><div class="line">%  Instructions: Fill in the variable called &quot;patches&quot; using data </div><div class="line">%  from IMAGES.  </div><div class="line">%  </div><div class="line">%  IMAGES is a 3D array containing 10 images</div><div class="line">%  For instance, IMAGES(:,:,6) is a 512x512 array containing the 6th image,</div><div class="line">%  and you can type &quot;imagesc(IMAGES(:,:,6)), colormap gray;&quot; to visualize</div><div class="line">%  it. (The contrast（对比度） on these images look a bit off because they have</div><div class="line">%  been preprocessed using using &quot;whitening（白化）.&quot;  See the lecture notes for</div><div class="line">%  more details.) As a second example, IMAGES(21:30,21:30,1) is an image</div><div class="line">%  patch（图像片） corresponding to the pixels in the block (21,21) to (30,30) of</div><div class="line">%  Image 1</div><div class="line"></div><div class="line">%% 注释译文：</div><div class="line">%   指导：用IMAGES中提取出的数据填充变量&quot;patches&quot;</div><div class="line">%   </div><div class="line">%   IMAGES是一个包含10个图片的3D向量</div><div class="line">%   举个栗子：</div><div class="line">%   IMAGES(:,:,6)是一个包含第6张图片的512*512的向量，</div><div class="line">%   输入“imagesc(IMAGES(:,:,6)),colormap gray;”能显示出该图片。</div><div class="line">%   图片对比度比较低，因为图片经过了白化预处理，教程笔记中有关于白化的更多细节</div><div class="line">%   第二个栗子：</div><div class="line">%   IMAGES(21:30,21:30,1)是第1张图片里从像素点(21,21)到(30,30)的像素块组成的图像片</div><div class="line"></div><div class="line">%%</div><div class="line">for imageNum = 1:10 % 在每张图片中选取1000个patch，共10000个patch</div><div class="line">    [rowNum, colNum] = size(IMAGES(:, :, imageNum)); % 返回每张图片的尺寸，保存在rowNum和colNum中</div><div class="line">    for patchNum = 1:1000 % 每个patch的尺寸为8*8</div><div class="line">        xPos = randi([1, rowNum-patchsize+1]); % 设置xPos为1和图像最大行数之间的一个随机数</div><div class="line">        yPos = randi([1, colNum-patchsize+1]); % 设置yPos为1和图像最大列数之间的一个随机数</div><div class="line">        patches(:, (imageNum-1)*1000+patchNum) =  reshape(IMAGES(xPos:xPos+7, yPos:yPos+7, imageNum), 64, 1);</div><div class="line">        % reshape将提取出的8*8的图像变形为64*1的列向量</div><div class="line">        % patches选中矩阵patches相应的列放入64*1的列向量</div><div class="line">    end</div><div class="line">end</div><div class="line">        </div><div class="line"></div><div class="line"></div><div class="line">%% ---------------------------------------------------------------</div><div class="line">% For the autoencoder to work well we need to normalize the data</div><div class="line">% Specifically, since the output of the network is bounded between [0,1]</div><div class="line">% (due to the sigmoid activation function), we have to make sure </div><div class="line">% the range of pixel values is also bounded between [0,1]</div><div class="line">patches = normalizeData(patches);</div><div class="line"></div><div class="line">end</div><div class="line"></div><div class="line"></div><div class="line">%% ---------------------------------------------------------------</div><div class="line">function patches = normalizeData(patches)</div><div class="line"></div><div class="line">% Squash data to [0.1, 0.9] since we use sigmoid as the activation</div><div class="line">% function in the output layer</div><div class="line"></div><div class="line">% Remove DC (mean of images). </div><div class="line">patches = bsxfun(@minus, patches, mean(patches));</div><div class="line"></div><div class="line">% Truncate to +/-3 standard deviations and scale to -1 to 1</div><div class="line">pstd = 3 * std(patches(:));</div><div class="line">patches = max(min(patches, pstd), -pstd) / pstd;</div><div class="line"></div><div class="line">% Rescale from [-1,1] to [0.1,0.9]</div><div class="line">patches = (patches + 1) * 0.4 + 0.1;</div><div class="line"></div><div class="line">end</div></pre></td></tr></table></figure></p>
<p>&emsp;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div></pre></td><td class="code"><pre><div class="line">% sparseAutoencoderCost.m</div><div class="line">function [cost,grad] = sparseAutoencoderCost(theta, visibleSize, hiddenSize, ...</div><div class="line">                                             lambda, sparsityParam, beta, data)</div><div class="line">%%</div><div class="line">% 本函数返回代价函数以及一次迭代后更新权重的变化值</div><div class="line">%%</div><div class="line">% visibleSize: the number of input units (probably 64) </div><div class="line">% hiddenSize: the number of hidden units (probably 25) </div><div class="line">% lambda: weight decay parameter 权重衰减参数</div><div class="line">% sparsityParam: The desired average activation for the hidden units (denoted in the lecture</div><div class="line">%                           notes by the greek alphabet rho, which looks</div><div class="line">%                           like a lower-case &quot;p&quot;). 隐藏层期望激活度，在教程中用rho表示</div><div class="line">% beta: weight of sparsity penalty term 稀疏性惩罚项的权重，在教程中用beta表示</div><div class="line">% data: Our 64x10000 matrix containing the training data.  So, data(:,i) is the i-th training example. </div><div class="line">  </div><div class="line">% The input theta is a vector (because minFunc expects the parameters to be</div><div class="line">% a vector). </div><div class="line">% 输入的theta是向量</div><div class="line">% theta是由所有参数组合起来的一个s1*s2+s2*s3+s2+s3维的向量</div><div class="line">% 之所以要将参数转换成这样的格式，是因为要满足minFunc函数的参数</div><div class="line"></div><div class="line">% We first convert theta to the (W1, W2, b1, b2) matrix/vector format, so that this </div><div class="line">% follows the notation convention（记符惯例） of the lecture notes.</div><div class="line">% 我们先将theta转换为(W1,W2,b1,b2)的矩阵/向量格式，符合教程笔记的记符惯例</div><div class="line"></div><div class="line">W1 = reshape(theta(1:hiddenSize*visibleSize), hiddenSize, visibleSize);</div><div class="line">% 将theta中第1到第s1*s2个元素变形组合成s2*s1维的矩阵，变形时使用向量按列开始填充</div><div class="line">W2 = reshape(theta(hiddenSize*visibleSize+1:2*hiddenSize*visibleSize), visibleSize, hiddenSize);</div><div class="line">% 将第s1*s2+1到第2*s2*s3个元素变形组合成s3*s3维矩阵，这里有s1=s3</div><div class="line">b1 = theta(2*hiddenSize*visibleSize+1:2*hiddenSize*visibleSize+hiddenSize);</div><div class="line">% 类似上面，初始化输入层偏置项向量b1</div><div class="line">b2 = theta(2*hiddenSize*visibleSize+hiddenSize+1:end);</div><div class="line">% 初始化第二层偏置项向量b2</div><div class="line"></div><div class="line">% Cost and gradient variables (your code needs to compute these values). </div><div class="line">% Here, we initialize them to zeros. </div><div class="line">cost = 0; % 将代价值初始化为0</div><div class="line">W1grad = zeros(size(W1)); </div><div class="line">W2grad = zeros(size(W2));</div><div class="line">b1grad = zeros(size(b1)); </div><div class="line">b2grad = zeros(size(b2));</div><div class="line"></div><div class="line">%% ---------- YOUR CODE HERE --------------------------------------</div><div class="line">%  Instructions: Compute the cost/optimization objective J_sparse(W,b) for the Sparse Autoencoder,</div><div class="line">%                and the corresponding gradients W1grad, W2grad, b1grad, b2grad.</div><div class="line">%</div><div class="line">% W1grad, W2grad, b1grad and b2grad should be computed using backpropagation.</div><div class="line">% Note that W1grad has the same dimensions as W1, b1grad has the same dimensions</div><div class="line">% as b1, etc.  Your code should set W1grad to be the partial derivative of J_sparse(W,b) with</div><div class="line">% respect to W1.  I.e., W1grad(i,j) should be the partial derivative of J_sparse(W,b) </div><div class="line">% with respect to the input parameter W1(i,j).  Thus, W1grad should be equal to the term </div><div class="line">% [(1/m) \Delta W^&#123;(1)&#125; + \lambda W^&#123;(1)&#125;] in the last block of pseudo-code in Section 2.2 </div><div class="line">% of the lecture notes (and similarly for W2grad, b1grad, b2grad).</div><div class="line">% </div><div class="line">% Stated differently, if we were using batch gradient descent to optimize the parameters,</div><div class="line">% the gradient descent update to W1 would be W1 := W1 - alpha * W1grad, and similarly for W2, b1, b2. </div><div class="line">% </div><div class="line"></div><div class="line">%%</div><div class="line">% </div><div class="line">%注释译文</div><div class="line">% 指导：为稀疏自编码器计算代价/优化对象J_sparse(W,b)，以及对应的梯度W1grad, W2grad, b1grad, b2grad</div><div class="line">% </div><div class="line">% W1grad, W2grad, b1grad和b2grad应该使用反向传导计算。</div><div class="line">% 注意W1grad和W1有相同的维度，b1grad和b1有相同的维度等。</div><div class="line">% 你的代码应该设置W1grad为J_sparse(W,b)关于W1的偏导数，</div><div class="line">% 也就是说，W1grad(i,j)应该是J_sparse(W,b)关于输入参数W1(i,j)的偏导数。</div><div class="line">% 因此，W1grad应该等于项[(1/m) \Delta W^&#123;(1)&#125;+ \ambdaW^&#123;(1)&#125;]（其他参数也一样），</div><div class="line">% 参见课件Section2.2最后一块的伪代码</div><div class="line">% 换句话说，如果我们使用批量梯度下降法去最小化参数，</div><div class="line">% W1权值的更新应该是W1:= W1-alpha*W1grad, 而W2,b1,b2也类似</div><div class="line">%</div><div class="line"></div><div class="line">% J_sparse(W,b)共有三项</div><div class="line">Jcost = 0; % 样本均方误差项</div><div class="line">Jweight = 0; % 权重衰减项</div><div class="line">Jsparse = 0; % 稀疏性惩罚项</div><div class="line">[n m] = size(data); % n表示样本特征数，m表示样本个数</div><div class="line"></div><div class="line">% 前向算法计算各神经网络节点的线性组合值和active值</div><div class="line">z2 = W1*data + repmat(b1, 1, m); % 第二层输入值，即隐藏层中各节点输入值矩阵，repmat函数将矩阵进行重复并组合起来</div><div class="line">a2 = sigmoid(z2); % 第二层输出值（激活值），注意sigmoid函数在本文件164行定义</div><div class="line">z3 = W2*a2 + repmat(b2, 1, m); % 第三层输入值</div><div class="line">a3 = sigmoid(z3); % 输出层各节点输出值（激活值）矩阵</div><div class="line"></div><div class="line">% 计算样本均方误差项</div><div class="line">Jcost = (0.5/m)*sum(sum((a3-data).^2));</div><div class="line"></div><div class="line">% 计算权重衰减项</div><div class="line">Jweight = (1/2)*(sum(sum(W1.^2)) + sum(sum(W2.^2)));</div><div class="line"></div><div class="line">% 计算稀疏性惩罚项</div><div class="line">rho = (1/m).*sum(a2, 2); % 求出教程中的rho，即隐藏层（第二层）神经元的平均活跃度，</div><div class="line">                                        % 在训练集上取隐藏层每个节点输出值的平均</div><div class="line">Jsparse = sum(sparsityParam .* log(sparsityParam ./ rho) + ...</div><div class="line">              (1-sparsityParam) .* log((1-sparsityParam) ./ (1 - rho )));</div><div class="line">          </div><div class="line">% 代价函数总表达式</div><div class="line">cost = Jcost+lambda*Jweight+beta*Jsparse;</div><div class="line"></div><div class="line">% 计算残差</div><div class="line">d3 = -(data-a3).*sigmoidInv(z3); % 输出层（第三层）残差，sigmoidInv为sigmoid导数，在第170行有定义</div><div class="line">sterm = beta*(-sparsityParam./rho+(1-sparsityParam)./(1-rho)); % 隐藏层稀疏性规则项</div><div class="line">d2 = (W2&apos;*d3+repmat(sterm, 1, m)).*sigmoidInv(z2); % 隐藏层（第二层）残差，计算时要加入稀疏性规则项</div><div class="line"></div><div class="line">% 计算W1grad</div><div class="line">W1grad = W1grad+d2*data&apos;;</div><div class="line">W1grad = (1/m).*W1grad+lambda*W1;</div><div class="line"></div><div class="line">% 计算W2grad</div><div class="line">W2grad = W2grad+d3*a2&apos;;</div><div class="line">W2grad = (1/m).*W2grad+lambda*W2;</div><div class="line"></div><div class="line">% 计算b1grad</div><div class="line">b1grad = b1grad + sum(d2, 2); % 注意b的偏导数是一个向量，因此要把一行的值累加起来</div><div class="line">b1grad = (1/m)*b1grad;</div><div class="line"></div><div class="line">% 计算b2grad</div><div class="line">b2grad = b2grad + sum(d3, 2);</div><div class="line">b2grad = (1/m)*b2grad;</div><div class="line"></div><div class="line"></div><div class="line">%-------------------------------------------------------------------</div><div class="line">% After computing the cost and gradient, we will convert the gradients back</div><div class="line">% to a vector format (suitable for minFunc).  Specifically, we will unroll</div><div class="line">% your gradient matrices into a vector.</div><div class="line"></div><div class="line">grad = [W1grad(:) ; W2grad(:) ; b1grad(:) ; b2grad(:)]; %将各矩阵按列首尾相接成为一个列向量</div><div class="line"></div><div class="line">end</div><div class="line"></div><div class="line">%-------------------------------------------------------------------</div><div class="line">% Here&apos;s an implementation of the sigmoid function, which you may find useful</div><div class="line">% in your computation of the costs and the gradients.  This inputs a (row or</div><div class="line">% column) vector (say (z1, z2, z3)) and returns (f(z1), f(z2), f(z3)). </div><div class="line"></div><div class="line">% 定义sigmoid函数</div><div class="line">function sigm = sigmoid(x)</div><div class="line">  </div><div class="line">    sigm = 1 ./ (1 + exp(-x));</div><div class="line">end</div><div class="line"></div><div class="line">% sigmoid函数求导</div><div class="line">function sigmInv = sigmoidInv(x)</div><div class="line">    </div><div class="line">    sigmInv = sigmoid(x).*(1-sigmoid(x)); </div><div class="line">end</div></pre></td></tr></table></figure>
<p>&emsp;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">% computeNumericalGradient.m</div><div class="line">function numgrad = computeNumericalGradient(J, theta)</div><div class="line">% numgrad = computeNumericalGradient(J, theta)</div><div class="line">% theta: a vector of parameters</div><div class="line">% J: a function that outputs a real-number. Calling y = J(theta) will return the</div><div class="line">% function value at theta. </div><div class="line">  </div><div class="line">% Initialize numgrad with zeros</div><div class="line">numgrad = zeros(size(theta));</div><div class="line"></div><div class="line">%% ---------- YOUR CODE HERE --------------------------------------</div><div class="line">% Instructions: </div><div class="line">% Implement numerical gradient checking, and return the result in numgrad.  </div><div class="line">% (See Section 2.3 of the lecture notes.)</div><div class="line">% You should write code so that numgrad(i) is (the numerical approximation to) the </div><div class="line">% partial derivative of J with respect to the i-th input argument, evaluated at theta.  </div><div class="line">% I.e., numgrad(i) should be the (approximately) the partial derivative of J with </div><div class="line">% respect to theta(i).</div><div class="line">%                </div><div class="line">% Hint: You will probably want to compute the elements of numgrad one at a time. </div><div class="line"></div><div class="line">%% 注释译文</div><div class="line">% 指导：</div><div class="line">% 实现数值上的梯度检验，并将结果返回到numgrad中</div><div class="line">% （查看课程笔记的Section 2.3）</div><div class="line">% 你应当写代码使numgrad(i)是J关于第i个输入参数的偏导数的近似值，输入参数为theta</div><div class="line">% 也就是说，numgrad(i)应该近似是J关于theta(i)的偏导数</div><div class="line">%</div><div class="line">% 提示：你很可能希望一次计算一个numgrad中的元素</div><div class="line"></div><div class="line">epsilon = 1e-4;</div><div class="line">n = size(theta, 1); % n为向量theta的维度</div><div class="line">E = eye(n); % 函数eye(n)用来创建n维的单位矩阵</div><div class="line">for i = 1:n</div><div class="line">    delta = E(:, i)*epsilon;</div><div class="line">    numgrad(i) = (J(theta+delta)-J(theta-delta))/(2.0*epsilon);</div><div class="line">end</div><div class="line">  </div><div class="line">%% ---------------------------------------------------------------</div><div class="line">end</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yODAyOS80NjA2">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>


      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/04/29/深度学习笔记：主成分分析（PCA）——标准化、协方差、相关系数和协方差矩阵/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          深度学习笔记：主成分分析（PCA）——标准化、协方差、相关系数和协方差矩阵
        
      </div>
    </a>
  
  
    <a href="/2017/04/29/深度学习笔记：稀疏自编码器（3）——稀疏自编码算法/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">深度学习笔记：稀疏自编码器（3）——稀疏自编码算法</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
      <ol class="nav">None</ol>
    
    </div>
  </aside>
</section>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2017 Aaron Wu All Rights Reserved.
          
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/bootstrap.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
