<!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习 | Aaron Wu</title>
  <meta name="author" content="Aaron Wu">
  
  <meta name="description" content="本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见Exercise:Sparse Autoencoder，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习">
  <meta property="og:site_name" content="Aaron Wu">

  
    <meta property="og:image" content="undefined">
  

  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/cerulean.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
    <script src="/js/marked.js"></script>
    <script src="/js/comment.js"></script>
    <script src="/js/timeago.min.js"></script>
    <script src="/js/highlight.min.js"></script>
	<script src="/js/spin.min.js"></script>
  
  <!-- analytics --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  



</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">Aaron Wu</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      


	
		<div class="page-header page-header-inverse ">		
			<h1 class="title title-inverse "> 深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习</h1>
		</div>		
	






<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  
		 <div class="alert alert-success description">
			<i class="fa fa-info-circle"></i> <p>本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见Exercise:Sparse Autoencoder，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。</p>
			
		 </div> <!-- alert -->
	  		

	  <p>&emsp;&emsp;本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见<a href="http://deeplearning.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder" target="_blank" rel="noopener">Exercise:Sparse Autoencoder</a>，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。<br><a id="more"></a></p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% sampleIMAGES.m</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">patches</span> = <span class="title">sampleIMAGES</span><span class="params">()</span> % 函数返回64*10000的矩阵<span class="title">patches</span></span></span><br><span class="line"><span class="comment">% sampleIMAGES</span></span><br><span class="line"><span class="comment">% Returns 10000 patches for training</span></span><br><span class="line"></span><br><span class="line">load IMAGES;    <span class="comment">% load images from disk </span></span><br><span class="line"></span><br><span class="line">patchsize = <span class="number">8</span>;  <span class="comment">% we'll use 8x8 patches </span></span><br><span class="line">numpatches = <span class="number">10000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize patches with zeros.  Your code will fill in this matrix--one</span></span><br><span class="line"><span class="comment">% column per patch, 10000 columns. </span></span><br><span class="line">patches = <span class="built_in">zeros</span>(patchsize*patchsize, numpatches);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------- YOUR CODE HERE --------------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Fill in the variable called "patches" using data </span></span><br><span class="line"><span class="comment">%  from IMAGES.  </span></span><br><span class="line"><span class="comment">%  </span></span><br><span class="line"><span class="comment">%  IMAGES is a 3D array containing 10 images</span></span><br><span class="line"><span class="comment">%  For instance, IMAGES(:,:,6) is a 512x512 array containing the 6th image,</span></span><br><span class="line"><span class="comment">%  and you can type "imagesc(IMAGES(:,:,6)), colormap gray;" to visualize</span></span><br><span class="line"><span class="comment">%  it. (The contrast（对比度） on these images look a bit off because they have</span></span><br><span class="line"><span class="comment">%  been preprocessed using using "whitening（白化）."  See the lecture notes for</span></span><br><span class="line"><span class="comment">%  more details.) As a second example, IMAGES(21:30,21:30,1) is an image</span></span><br><span class="line"><span class="comment">%  patch（图像片） corresponding to the pixels in the block (21,21) to (30,30) of</span></span><br><span class="line"><span class="comment">%  Image 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 注释译文：</span></span><br><span class="line"><span class="comment">%   指导：用IMAGES中提取出的数据填充变量"patches"</span></span><br><span class="line"><span class="comment">%   </span></span><br><span class="line"><span class="comment">%   IMAGES是一个包含10个图片的3D向量</span></span><br><span class="line"><span class="comment">%   举个栗子：</span></span><br><span class="line"><span class="comment">%   IMAGES(:,:,6)是一个包含第6张图片的512*512的向量，</span></span><br><span class="line"><span class="comment">%   输入“imagesc(IMAGES(:,:,6)),colormap gray;”能显示出该图片。</span></span><br><span class="line"><span class="comment">%   图片对比度比较低，因为图片经过了白化预处理，教程笔记中有关于白化的更多细节</span></span><br><span class="line"><span class="comment">%   第二个栗子：</span></span><br><span class="line"><span class="comment">%   IMAGES(21:30,21:30,1)是第1张图片里从像素点(21,21)到(30,30)的像素块组成的图像片</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="keyword">for</span> imageNum = <span class="number">1</span>:<span class="number">10</span> <span class="comment">% 在每张图片中选取1000个patch，共10000个patch</span></span><br><span class="line">    [rowNum, colNum] = <span class="built_in">size</span>(IMAGES(:, :, imageNum)); <span class="comment">% 返回每张图片的尺寸，保存在rowNum和colNum中</span></span><br><span class="line">    <span class="keyword">for</span> patchNum = <span class="number">1</span>:<span class="number">1000</span> <span class="comment">% 每个patch的尺寸为8*8</span></span><br><span class="line">        xPos = randi([<span class="number">1</span>, rowNum-patchsize+<span class="number">1</span>]); <span class="comment">% 设置xPos为1和图像最大行数之间的一个随机数</span></span><br><span class="line">        yPos = randi([<span class="number">1</span>, colNum-patchsize+<span class="number">1</span>]); <span class="comment">% 设置yPos为1和图像最大列数之间的一个随机数</span></span><br><span class="line">        patches(:, (imageNum<span class="number">-1</span>)*<span class="number">1000</span>+patchNum) =  <span class="built_in">reshape</span>(IMAGES(xPos:xPos+<span class="number">7</span>, yPos:yPos+<span class="number">7</span>, imageNum), <span class="number">64</span>, <span class="number">1</span>);</span><br><span class="line">        <span class="comment">% reshape将提取出的8*8的图像变形为64*1的列向量</span></span><br><span class="line">        <span class="comment">% patches选中矩阵patches相应的列放入64*1的列向量</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% For the autoencoder to work well we need to normalize the data</span></span><br><span class="line"><span class="comment">% Specifically, since the output of the network is bounded between [0,1]</span></span><br><span class="line"><span class="comment">% (due to the sigmoid activation function), we have to make sure </span></span><br><span class="line"><span class="comment">% the range of pixel values is also bounded between [0,1]</span></span><br><span class="line">patches = normalizeData(patches);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------------------------------------------------------------</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">patches</span> = <span class="title">normalizeData</span><span class="params">(patches)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Squash data to [0.1, 0.9] since we use sigmoid as the activation</span></span><br><span class="line"><span class="comment">% function in the output layer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Remove DC (mean of images). </span></span><br><span class="line">patches = <span class="built_in">bsxfun</span>(@minus, patches, <span class="built_in">mean</span>(patches));</span><br><span class="line"></span><br><span class="line"><span class="comment">% Truncate to +/-3 standard deviations and scale to -1 to 1</span></span><br><span class="line">pstd = <span class="number">3</span> * std(patches(:));</span><br><span class="line">patches = <span class="built_in">max</span>(<span class="built_in">min</span>(patches, pstd), -pstd) / pstd;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Rescale from [-1,1] to [0.1,0.9]</span></span><br><span class="line">patches = (patches + <span class="number">1</span>) * <span class="number">0.4</span> + <span class="number">0.1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% sparseAutoencoderCost.m</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[cost,grad]</span> = <span class="title">sparseAutoencoderCost</span><span class="params">(theta, visibleSize, hiddenSize, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                             lambda, sparsityParam, beta, data)</span></span></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% 本函数返回代价函数以及一次迭代后更新权重的变化值</span></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% visibleSize: the number of input units (probably 64) </span></span><br><span class="line"><span class="comment">% hiddenSize: the number of hidden units (probably 25) </span></span><br><span class="line"><span class="comment">% lambda: weight decay parameter 权重衰减参数</span></span><br><span class="line"><span class="comment">% sparsityParam: The desired average activation for the hidden units (denoted in the lecture</span></span><br><span class="line"><span class="comment">%                           notes by the greek alphabet rho, which looks</span></span><br><span class="line"><span class="comment">%                           like a lower-case "p"). 隐藏层期望激活度，在教程中用rho表示</span></span><br><span class="line"><span class="comment">% beta: weight of sparsity penalty term 稀疏性惩罚项的权重，在教程中用beta表示</span></span><br><span class="line"><span class="comment">% data: Our 64x10000 matrix containing the training data.  So, data(:,i) is the i-th training example. </span></span><br><span class="line">  </span><br><span class="line"><span class="comment">% The input theta is a vector (because minFunc expects the parameters to be</span></span><br><span class="line"><span class="comment">% a vector). </span></span><br><span class="line"><span class="comment">% 输入的theta是向量</span></span><br><span class="line"><span class="comment">% theta是由所有参数组合起来的一个s1*s2+s2*s3+s2+s3维的向量</span></span><br><span class="line"><span class="comment">% 之所以要将参数转换成这样的格式，是因为要满足minFunc函数的参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% We first convert theta to the (W1, W2, b1, b2) matrix/vector format, so that this </span></span><br><span class="line"><span class="comment">% follows the notation convention（记符惯例） of the lecture notes.</span></span><br><span class="line"><span class="comment">% 我们先将theta转换为(W1,W2,b1,b2)的矩阵/向量格式，符合教程笔记的记符惯例</span></span><br><span class="line"></span><br><span class="line">W1 = <span class="built_in">reshape</span>(theta(<span class="number">1</span>:hiddenSize*visibleSize), hiddenSize, visibleSize);</span><br><span class="line"><span class="comment">% 将theta中第1到第s1*s2个元素变形组合成s2*s1维的矩阵，变形时使用向量按列开始填充</span></span><br><span class="line">W2 = <span class="built_in">reshape</span>(theta(hiddenSize*visibleSize+<span class="number">1</span>:<span class="number">2</span>*hiddenSize*visibleSize), visibleSize, hiddenSize);</span><br><span class="line"><span class="comment">% 将第s1*s2+1到第2*s2*s3个元素变形组合成s3*s3维矩阵，这里有s1=s3</span></span><br><span class="line">b1 = theta(<span class="number">2</span>*hiddenSize*visibleSize+<span class="number">1</span>:<span class="number">2</span>*hiddenSize*visibleSize+hiddenSize);</span><br><span class="line"><span class="comment">% 类似上面，初始化输入层偏置项向量b1</span></span><br><span class="line">b2 = theta(<span class="number">2</span>*hiddenSize*visibleSize+hiddenSize+<span class="number">1</span>:<span class="keyword">end</span>);</span><br><span class="line"><span class="comment">% 初始化第二层偏置项向量b2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Cost and gradient variables (your code needs to compute these values). </span></span><br><span class="line"><span class="comment">% Here, we initialize them to zeros. </span></span><br><span class="line">cost = <span class="number">0</span>; <span class="comment">% 将代价值初始化为0</span></span><br><span class="line">W1grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(W1)); </span><br><span class="line">W2grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(W2));</span><br><span class="line">b1grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(b1)); </span><br><span class="line">b2grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(b2));</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------- YOUR CODE HERE --------------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Compute the cost/optimization objective J_sparse(W,b) for the Sparse Autoencoder,</span></span><br><span class="line"><span class="comment">%                and the corresponding gradients W1grad, W2grad, b1grad, b2grad.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% W1grad, W2grad, b1grad and b2grad should be computed using backpropagation.</span></span><br><span class="line"><span class="comment">% Note that W1grad has the same dimensions as W1, b1grad has the same dimensions</span></span><br><span class="line"><span class="comment">% as b1, etc.  Your code should set W1grad to be the partial derivative of J_sparse(W,b) with</span></span><br><span class="line"><span class="comment">% respect to W1.  I.e., W1grad(i,j) should be the partial derivative of J_sparse(W,b) </span></span><br><span class="line"><span class="comment">% with respect to the input parameter W1(i,j).  Thus, W1grad should be equal to the term </span></span><br><span class="line"><span class="comment">% [(1/m) \Delta W^&#123;(1)&#125; + \lambda W^&#123;(1)&#125;] in the last block of pseudo-code in Section 2.2 </span></span><br><span class="line"><span class="comment">% of the lecture notes (and similarly for W2grad, b1grad, b2grad).</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% Stated differently, if we were using batch gradient descent to optimize the parameters,</span></span><br><span class="line"><span class="comment">% the gradient descent update to W1 would be W1 := W1 - alpha * W1grad, and similarly for W2, b1, b2. </span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">%注释译文</span></span><br><span class="line"><span class="comment">% 指导：为稀疏自编码器计算代价/优化对象J_sparse(W,b)，以及对应的梯度W1grad, W2grad, b1grad, b2grad</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% W1grad, W2grad, b1grad和b2grad应该使用反向传导计算。</span></span><br><span class="line"><span class="comment">% 注意W1grad和W1有相同的维度，b1grad和b1有相同的维度等。</span></span><br><span class="line"><span class="comment">% 你的代码应该设置W1grad为J_sparse(W,b)关于W1的偏导数，</span></span><br><span class="line"><span class="comment">% 也就是说，W1grad(i,j)应该是J_sparse(W,b)关于输入参数W1(i,j)的偏导数。</span></span><br><span class="line"><span class="comment">% 因此，W1grad应该等于项[(1/m) \Delta W^&#123;(1)&#125;+ \ambdaW^&#123;(1)&#125;]（其他参数也一样），</span></span><br><span class="line"><span class="comment">% 参见课件Section2.2最后一块的伪代码</span></span><br><span class="line"><span class="comment">% 换句话说，如果我们使用批量梯度下降法去最小化参数，</span></span><br><span class="line"><span class="comment">% W1权值的更新应该是W1:= W1-alpha*W1grad, 而W2,b1,b2也类似</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% J_sparse(W,b)共有三项</span></span><br><span class="line">Jcost = <span class="number">0</span>; <span class="comment">% 样本均方误差项</span></span><br><span class="line">Jweight = <span class="number">0</span>; <span class="comment">% 权重衰减项</span></span><br><span class="line">Jsparse = <span class="number">0</span>; <span class="comment">% 稀疏性惩罚项</span></span><br><span class="line">[n m] = <span class="built_in">size</span>(data); <span class="comment">% n表示样本特征数，m表示样本个数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 前向算法计算各神经网络节点的线性组合值和active值</span></span><br><span class="line">z2 = W1*data + <span class="built_in">repmat</span>(b1, <span class="number">1</span>, m); <span class="comment">% 第二层输入值，即隐藏层中各节点输入值矩阵，repmat函数将矩阵进行重复并组合起来</span></span><br><span class="line">a2 = sigmoid(z2); <span class="comment">% 第二层输出值（激活值），注意sigmoid函数在本文件164行定义</span></span><br><span class="line">z3 = W2*a2 + <span class="built_in">repmat</span>(b2, <span class="number">1</span>, m); <span class="comment">% 第三层输入值</span></span><br><span class="line">a3 = sigmoid(z3); <span class="comment">% 输出层各节点输出值（激活值）矩阵</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算样本均方误差项</span></span><br><span class="line">Jcost = (<span class="number">0.5</span>/m)*sum(sum((a3-data).^<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算权重衰减项</span></span><br><span class="line">Jweight = (<span class="number">1</span>/<span class="number">2</span>)*(sum(sum(W1.^<span class="number">2</span>)) + sum(sum(W2.^<span class="number">2</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算稀疏性惩罚项</span></span><br><span class="line">rho = (<span class="number">1</span>/m).*sum(a2, <span class="number">2</span>); <span class="comment">% 求出教程中的rho，即隐藏层（第二层）神经元的平均活跃度，</span></span><br><span class="line">                                        <span class="comment">% 在训练集上取隐藏层每个节点输出值的平均</span></span><br><span class="line">Jsparse = sum(sparsityParam .* <span class="built_in">log</span>(sparsityParam ./ rho) + ...</span><br><span class="line">              (<span class="number">1</span>-sparsityParam) .* <span class="built_in">log</span>((<span class="number">1</span>-sparsityParam) ./ (<span class="number">1</span> - rho )));</span><br><span class="line">          </span><br><span class="line"><span class="comment">% 代价函数总表达式</span></span><br><span class="line">cost = Jcost+lambda*Jweight+<span class="built_in">beta</span>*Jsparse;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算残差</span></span><br><span class="line">d3 = -(data-a3).*sigmoidInv(z3); <span class="comment">% 输出层（第三层）残差，sigmoidInv为sigmoid导数，在第170行有定义</span></span><br><span class="line">sterm = <span class="built_in">beta</span>*(-sparsityParam./rho+(<span class="number">1</span>-sparsityParam)./(<span class="number">1</span>-rho)); <span class="comment">% 隐藏层稀疏性规则项</span></span><br><span class="line">d2 = (W2'*d3+<span class="built_in">repmat</span>(sterm, <span class="number">1</span>, m)).*sigmoidInv(z2); <span class="comment">% 隐藏层（第二层）残差，计算时要加入稀疏性规则项</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算W1grad</span></span><br><span class="line">W1grad = W1grad+d2*data';</span><br><span class="line">W1grad = (<span class="number">1</span>/m).*W1grad+lambda*W1;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算W2grad</span></span><br><span class="line">W2grad = W2grad+d3*a2';</span><br><span class="line">W2grad = (<span class="number">1</span>/m).*W2grad+lambda*W2;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算b1grad</span></span><br><span class="line">b1grad = b1grad + sum(d2, <span class="number">2</span>); <span class="comment">% 注意b的偏导数是一个向量，因此要把一行的值累加起来</span></span><br><span class="line">b1grad = (<span class="number">1</span>/m)*b1grad;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算b2grad</span></span><br><span class="line">b2grad = b2grad + sum(d3, <span class="number">2</span>);</span><br><span class="line">b2grad = (<span class="number">1</span>/m)*b2grad;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%-------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% After computing the cost and gradient, we will convert the gradients back</span></span><br><span class="line"><span class="comment">% to a vector format (suitable for minFunc).  Specifically, we will unroll</span></span><br><span class="line"><span class="comment">% your gradient matrices into a vector.</span></span><br><span class="line"></span><br><span class="line">grad = [W1grad(:) ; W2grad(:) ; b1grad(:) ; b2grad(:)]; <span class="comment">%将各矩阵按列首尾相接成为一个列向量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%-------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% Here's an implementation of the sigmoid function, which you may find useful</span></span><br><span class="line"><span class="comment">% in your computation of the costs and the gradients.  This inputs a (row or</span></span><br><span class="line"><span class="comment">% column) vector (say (z1, z2, z3)) and returns (f(z1), f(z2), f(z3)). </span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 定义sigmoid函数</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sigm</span> = <span class="title">sigmoid</span><span class="params">(x)</span></span></span><br><span class="line">  </span><br><span class="line">    sigm = <span class="number">1</span> ./ (<span class="number">1</span> + <span class="built_in">exp</span>(-x));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% sigmoid函数求导</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sigmInv</span> = <span class="title">sigmoidInv</span><span class="params">(x)</span></span></span><br><span class="line">    </span><br><span class="line">    sigmInv = sigmoid(x).*(<span class="number">1</span>-sigmoid(x)); </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% computeNumericalGradient.m</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">numgrad</span> = <span class="title">computeNumericalGradient</span><span class="params">(J, theta)</span></span></span><br><span class="line"><span class="comment">% numgrad = computeNumericalGradient(J, theta)</span></span><br><span class="line"><span class="comment">% theta: a vector of parameters</span></span><br><span class="line"><span class="comment">% J: a function that outputs a real-number. Calling y = J(theta) will return the</span></span><br><span class="line"><span class="comment">% function value at theta. </span></span><br><span class="line">  </span><br><span class="line"><span class="comment">% Initialize numgrad with zeros</span></span><br><span class="line">numgrad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------- YOUR CODE HERE --------------------------------------</span></span><br><span class="line"><span class="comment">% Instructions: </span></span><br><span class="line"><span class="comment">% Implement numerical gradient checking, and return the result in numgrad.  </span></span><br><span class="line"><span class="comment">% (See Section 2.3 of the lecture notes.)</span></span><br><span class="line"><span class="comment">% You should write code so that numgrad(i) is (the numerical approximation to) the </span></span><br><span class="line"><span class="comment">% partial derivative of J with respect to the i-th input argument, evaluated at theta.  </span></span><br><span class="line"><span class="comment">% I.e., numgrad(i) should be the (approximately) the partial derivative of J with </span></span><br><span class="line"><span class="comment">% respect to theta(i).</span></span><br><span class="line"><span class="comment">%                </span></span><br><span class="line"><span class="comment">% Hint: You will probably want to compute the elements of numgrad one at a time. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 注释译文</span></span><br><span class="line"><span class="comment">% 指导：</span></span><br><span class="line"><span class="comment">% 实现数值上的梯度检验，并将结果返回到numgrad中</span></span><br><span class="line"><span class="comment">% （查看课程笔记的Section 2.3）</span></span><br><span class="line"><span class="comment">% 你应当写代码使numgrad(i)是J关于第i个输入参数的偏导数的近似值，输入参数为theta</span></span><br><span class="line"><span class="comment">% 也就是说，numgrad(i)应该近似是J关于theta(i)的偏导数</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% 提示：你很可能希望一次计算一个numgrad中的元素</span></span><br><span class="line"></span><br><span class="line">epsilon = <span class="number">1e-4</span>;</span><br><span class="line">n = <span class="built_in">size</span>(theta, <span class="number">1</span>); <span class="comment">% n为向量theta的维度</span></span><br><span class="line">E = <span class="built_in">eye</span>(n); <span class="comment">% 函数eye(n)用来创建n维的单位矩阵</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n</span><br><span class="line">    delta = E(:, <span class="built_in">i</span>)*epsilon;</span><br><span class="line">    numgrad(<span class="built_in">i</span>) = (J(theta+delta)-J(theta-delta))/(<span class="number">2.0</span>*epsilon);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">%% ---------------------------------------------------------------</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
	  
	</div>

	<!-- recommended posts -->
	

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
    	<li class="prev"><a href="/2017/04/29/深度学习笔记：主成分分析（PCA）——标准化、协方差、相关系数和协方差矩阵/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>上一页</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
		   <li class="next"><a href="/2017/04/29/深度学习笔记：稀疏自编码器（3）——稀疏自编码算法/" class="alignright next">下一页<i class="fa fa-arrow-circle-o-right"></i></a></li>         
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">留言</h2>
    	 
	 <div id="comment-thread"></div>
	 <div id="loading-spin"></div>
	 <script type="text/javascript">
	   getComments({
           type: "github" ? "github" : "github",       
	       user: "wzpan",
	       repo: "hexo-theme-freemind-blog",
		   client_id: "bf7d4ba11877db88543e",
           client_secret: "bff8a6b06b745c0bfcdccbe225623ea8e2a057bb",
		   no_comment: "暂时还没有留言呢，点击下面的按钮去留言吧！",
		   go_to_comment: "去留言",
		   no_issue: "no_issue",
		   issue_title: "深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习",
		   issue_id: "undefined",
		   btn_class: "btn btn-primary",
		   comments_target: "#comment-thread",
		   loading_target: "#loading_spin"
		   });
	 </script>
  
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2017-04-29 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/机器学习/">机器学习<span>6</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/机器学习/">机器学习<span>6</span></a></li>
    </ul>
	</div>
	

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



    </div>
  </div>
  <div class="container-narrow">
    <footer> <p>
  &copy; 2020 Aaron Wu
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>




<!-- syntax highlighting -->

  <script>
  marked.setOptions({
    highlight: function (code, lang) {
        return hljs.highlightAuto(code).value;
    }
  });
  function Highlighting(){
    var markdowns = document.getElementsByClassName('markdown');
    for(var i=0;i<markdowns.length;i++){
        if(markdowns[i].innerHTML) markdowns[i].innerHTML =marked(markdowns[i].innerHTML);
    }
  }
  window.addEventListener('DOMContentLoaded', Highlighting, false);
  window.addEventListener('load', Highlighting, false);
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>