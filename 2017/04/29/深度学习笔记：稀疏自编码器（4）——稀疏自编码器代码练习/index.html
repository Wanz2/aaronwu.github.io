
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习 | Aaron Wu</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Aaron Wu">
    

    
    <meta name="description" content="本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见Exercise:Sparse Autoencoder，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习">
<meta property="og:url" content="https://wanz2.github.io/2017/04/29/深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习/index.html">
<meta property="og:site_name" content="Aaron Wu">
<meta property="og:description" content="本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见Exercise:Sparse Autoencoder，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-05-23T16:21:36.164Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习">
<meta name="twitter:description" content="本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见Exercise:Sparse Autoencoder，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。">

    
    <link rel="alternative" href="/atom.xml" title="Aaron Wu" type="application/atom+xml">
    
    
    
    <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
</html>
  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Aaron Wu">Aaron Wu</a></h1>
				<h2 class="blog-motto">Strive for excellence, not perfection.</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
						<li><a href="/tags">标签</a></li>
					
						<li><a href="/categories">分类</a></li>
					
						<li><a href="/about">关于</a></li>
					
					<li>
 					
						<form class="search" action="/search" target="_blank">
							<label>Search</label>
						<input name="s" type="hidden" value= 3996866491135379000 ><input type="text" name="q" size="30" placeholder="搜索"><br>
						</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/29/深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习/" title="深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习" itemprop="url">深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Aaron Wu" target="_blank" itemprop="author">Aaron Wu</a>
		
  <p class="article-time">
    <time datetime="2017-04-29T23:45:57.000Z" itemprop="datePublished"> 发表于 2017-04-29</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			
		
		</div>
		
		<p>&emsp;&emsp;本文是ufldl稀疏自编码器教程中的练习，详细的练习要求参见<a href="http://deeplearning.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder" target="_blank" rel="noopener">Exercise:Sparse Autoencoder</a>，笔者将练习中要求实现的三个文件代码记录在此，并作了详细的注释。<br><a id="more"></a></p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% sampleIMAGES.m</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">patches</span> = <span class="title">sampleIMAGES</span><span class="params">()</span> % 函数返回64*10000的矩阵<span class="title">patches</span></span></span><br><span class="line"><span class="comment">% sampleIMAGES</span></span><br><span class="line"><span class="comment">% Returns 10000 patches for training</span></span><br><span class="line"></span><br><span class="line">load IMAGES;    <span class="comment">% load images from disk </span></span><br><span class="line"></span><br><span class="line">patchsize = <span class="number">8</span>;  <span class="comment">% we'll use 8x8 patches </span></span><br><span class="line">numpatches = <span class="number">10000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize patches with zeros.  Your code will fill in this matrix--one</span></span><br><span class="line"><span class="comment">% column per patch, 10000 columns. </span></span><br><span class="line">patches = <span class="built_in">zeros</span>(patchsize*patchsize, numpatches);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------- YOUR CODE HERE --------------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Fill in the variable called "patches" using data </span></span><br><span class="line"><span class="comment">%  from IMAGES.  </span></span><br><span class="line"><span class="comment">%  </span></span><br><span class="line"><span class="comment">%  IMAGES is a 3D array containing 10 images</span></span><br><span class="line"><span class="comment">%  For instance, IMAGES(:,:,6) is a 512x512 array containing the 6th image,</span></span><br><span class="line"><span class="comment">%  and you can type "imagesc(IMAGES(:,:,6)), colormap gray;" to visualize</span></span><br><span class="line"><span class="comment">%  it. (The contrast（对比度） on these images look a bit off because they have</span></span><br><span class="line"><span class="comment">%  been preprocessed using using "whitening（白化）."  See the lecture notes for</span></span><br><span class="line"><span class="comment">%  more details.) As a second example, IMAGES(21:30,21:30,1) is an image</span></span><br><span class="line"><span class="comment">%  patch（图像片） corresponding to the pixels in the block (21,21) to (30,30) of</span></span><br><span class="line"><span class="comment">%  Image 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 注释译文：</span></span><br><span class="line"><span class="comment">%   指导：用IMAGES中提取出的数据填充变量"patches"</span></span><br><span class="line"><span class="comment">%   </span></span><br><span class="line"><span class="comment">%   IMAGES是一个包含10个图片的3D向量</span></span><br><span class="line"><span class="comment">%   举个栗子：</span></span><br><span class="line"><span class="comment">%   IMAGES(:,:,6)是一个包含第6张图片的512*512的向量，</span></span><br><span class="line"><span class="comment">%   输入“imagesc(IMAGES(:,:,6)),colormap gray;”能显示出该图片。</span></span><br><span class="line"><span class="comment">%   图片对比度比较低，因为图片经过了白化预处理，教程笔记中有关于白化的更多细节</span></span><br><span class="line"><span class="comment">%   第二个栗子：</span></span><br><span class="line"><span class="comment">%   IMAGES(21:30,21:30,1)是第1张图片里从像素点(21,21)到(30,30)的像素块组成的图像片</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="keyword">for</span> imageNum = <span class="number">1</span>:<span class="number">10</span> <span class="comment">% 在每张图片中选取1000个patch，共10000个patch</span></span><br><span class="line">    [rowNum, colNum] = <span class="built_in">size</span>(IMAGES(:, :, imageNum)); <span class="comment">% 返回每张图片的尺寸，保存在rowNum和colNum中</span></span><br><span class="line">    <span class="keyword">for</span> patchNum = <span class="number">1</span>:<span class="number">1000</span> <span class="comment">% 每个patch的尺寸为8*8</span></span><br><span class="line">        xPos = randi([<span class="number">1</span>, rowNum-patchsize+<span class="number">1</span>]); <span class="comment">% 设置xPos为1和图像最大行数之间的一个随机数</span></span><br><span class="line">        yPos = randi([<span class="number">1</span>, colNum-patchsize+<span class="number">1</span>]); <span class="comment">% 设置yPos为1和图像最大列数之间的一个随机数</span></span><br><span class="line">        patches(:, (imageNum<span class="number">-1</span>)*<span class="number">1000</span>+patchNum) =  <span class="built_in">reshape</span>(IMAGES(xPos:xPos+<span class="number">7</span>, yPos:yPos+<span class="number">7</span>, imageNum), <span class="number">64</span>, <span class="number">1</span>);</span><br><span class="line">        <span class="comment">% reshape将提取出的8*8的图像变形为64*1的列向量</span></span><br><span class="line">        <span class="comment">% patches选中矩阵patches相应的列放入64*1的列向量</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% For the autoencoder to work well we need to normalize the data</span></span><br><span class="line"><span class="comment">% Specifically, since the output of the network is bounded between [0,1]</span></span><br><span class="line"><span class="comment">% (due to the sigmoid activation function), we have to make sure </span></span><br><span class="line"><span class="comment">% the range of pixel values is also bounded between [0,1]</span></span><br><span class="line">patches = normalizeData(patches);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------------------------------------------------------------</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">patches</span> = <span class="title">normalizeData</span><span class="params">(patches)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Squash data to [0.1, 0.9] since we use sigmoid as the activation</span></span><br><span class="line"><span class="comment">% function in the output layer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Remove DC (mean of images). </span></span><br><span class="line">patches = <span class="built_in">bsxfun</span>(@minus, patches, <span class="built_in">mean</span>(patches));</span><br><span class="line"></span><br><span class="line"><span class="comment">% Truncate to +/-3 standard deviations and scale to -1 to 1</span></span><br><span class="line">pstd = <span class="number">3</span> * std(patches(:));</span><br><span class="line">patches = <span class="built_in">max</span>(<span class="built_in">min</span>(patches, pstd), -pstd) / pstd;</span><br><span class="line"></span><br><span class="line"><span class="comment">% Rescale from [-1,1] to [0.1,0.9]</span></span><br><span class="line">patches = (patches + <span class="number">1</span>) * <span class="number">0.4</span> + <span class="number">0.1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% sparseAutoencoderCost.m</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[cost,grad]</span> = <span class="title">sparseAutoencoderCost</span><span class="params">(theta, visibleSize, hiddenSize, ...</span></span></span><br><span class="line"><span class="function"><span class="params">                                             lambda, sparsityParam, beta, data)</span></span></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% 本函数返回代价函数以及一次迭代后更新权重的变化值</span></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% visibleSize: the number of input units (probably 64) </span></span><br><span class="line"><span class="comment">% hiddenSize: the number of hidden units (probably 25) </span></span><br><span class="line"><span class="comment">% lambda: weight decay parameter 权重衰减参数</span></span><br><span class="line"><span class="comment">% sparsityParam: The desired average activation for the hidden units (denoted in the lecture</span></span><br><span class="line"><span class="comment">%                           notes by the greek alphabet rho, which looks</span></span><br><span class="line"><span class="comment">%                           like a lower-case "p"). 隐藏层期望激活度，在教程中用rho表示</span></span><br><span class="line"><span class="comment">% beta: weight of sparsity penalty term 稀疏性惩罚项的权重，在教程中用beta表示</span></span><br><span class="line"><span class="comment">% data: Our 64x10000 matrix containing the training data.  So, data(:,i) is the i-th training example. </span></span><br><span class="line">  </span><br><span class="line"><span class="comment">% The input theta is a vector (because minFunc expects the parameters to be</span></span><br><span class="line"><span class="comment">% a vector). </span></span><br><span class="line"><span class="comment">% 输入的theta是向量</span></span><br><span class="line"><span class="comment">% theta是由所有参数组合起来的一个s1*s2+s2*s3+s2+s3维的向量</span></span><br><span class="line"><span class="comment">% 之所以要将参数转换成这样的格式，是因为要满足minFunc函数的参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% We first convert theta to the (W1, W2, b1, b2) matrix/vector format, so that this </span></span><br><span class="line"><span class="comment">% follows the notation convention（记符惯例） of the lecture notes.</span></span><br><span class="line"><span class="comment">% 我们先将theta转换为(W1,W2,b1,b2)的矩阵/向量格式，符合教程笔记的记符惯例</span></span><br><span class="line"></span><br><span class="line">W1 = <span class="built_in">reshape</span>(theta(<span class="number">1</span>:hiddenSize*visibleSize), hiddenSize, visibleSize);</span><br><span class="line"><span class="comment">% 将theta中第1到第s1*s2个元素变形组合成s2*s1维的矩阵，变形时使用向量按列开始填充</span></span><br><span class="line">W2 = <span class="built_in">reshape</span>(theta(hiddenSize*visibleSize+<span class="number">1</span>:<span class="number">2</span>*hiddenSize*visibleSize), visibleSize, hiddenSize);</span><br><span class="line"><span class="comment">% 将第s1*s2+1到第2*s2*s3个元素变形组合成s3*s3维矩阵，这里有s1=s3</span></span><br><span class="line">b1 = theta(<span class="number">2</span>*hiddenSize*visibleSize+<span class="number">1</span>:<span class="number">2</span>*hiddenSize*visibleSize+hiddenSize);</span><br><span class="line"><span class="comment">% 类似上面，初始化输入层偏置项向量b1</span></span><br><span class="line">b2 = theta(<span class="number">2</span>*hiddenSize*visibleSize+hiddenSize+<span class="number">1</span>:<span class="keyword">end</span>);</span><br><span class="line"><span class="comment">% 初始化第二层偏置项向量b2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Cost and gradient variables (your code needs to compute these values). </span></span><br><span class="line"><span class="comment">% Here, we initialize them to zeros. </span></span><br><span class="line">cost = <span class="number">0</span>; <span class="comment">% 将代价值初始化为0</span></span><br><span class="line">W1grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(W1)); </span><br><span class="line">W2grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(W2));</span><br><span class="line">b1grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(b1)); </span><br><span class="line">b2grad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(b2));</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------- YOUR CODE HERE --------------------------------------</span></span><br><span class="line"><span class="comment">%  Instructions: Compute the cost/optimization objective J_sparse(W,b) for the Sparse Autoencoder,</span></span><br><span class="line"><span class="comment">%                and the corresponding gradients W1grad, W2grad, b1grad, b2grad.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% W1grad, W2grad, b1grad and b2grad should be computed using backpropagation.</span></span><br><span class="line"><span class="comment">% Note that W1grad has the same dimensions as W1, b1grad has the same dimensions</span></span><br><span class="line"><span class="comment">% as b1, etc.  Your code should set W1grad to be the partial derivative of J_sparse(W,b) with</span></span><br><span class="line"><span class="comment">% respect to W1.  I.e., W1grad(i,j) should be the partial derivative of J_sparse(W,b) </span></span><br><span class="line"><span class="comment">% with respect to the input parameter W1(i,j).  Thus, W1grad should be equal to the term </span></span><br><span class="line"><span class="comment">% [(1/m) \Delta W^&#123;(1)&#125; + \lambda W^&#123;(1)&#125;] in the last block of pseudo-code in Section 2.2 </span></span><br><span class="line"><span class="comment">% of the lecture notes (and similarly for W2grad, b1grad, b2grad).</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% Stated differently, if we were using batch gradient descent to optimize the parameters,</span></span><br><span class="line"><span class="comment">% the gradient descent update to W1 would be W1 := W1 - alpha * W1grad, and similarly for W2, b1, b2. </span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">%注释译文</span></span><br><span class="line"><span class="comment">% 指导：为稀疏自编码器计算代价/优化对象J_sparse(W,b)，以及对应的梯度W1grad, W2grad, b1grad, b2grad</span></span><br><span class="line"><span class="comment">% </span></span><br><span class="line"><span class="comment">% W1grad, W2grad, b1grad和b2grad应该使用反向传导计算。</span></span><br><span class="line"><span class="comment">% 注意W1grad和W1有相同的维度，b1grad和b1有相同的维度等。</span></span><br><span class="line"><span class="comment">% 你的代码应该设置W1grad为J_sparse(W,b)关于W1的偏导数，</span></span><br><span class="line"><span class="comment">% 也就是说，W1grad(i,j)应该是J_sparse(W,b)关于输入参数W1(i,j)的偏导数。</span></span><br><span class="line"><span class="comment">% 因此，W1grad应该等于项[(1/m) \Delta W^&#123;(1)&#125;+ \ambdaW^&#123;(1)&#125;]（其他参数也一样），</span></span><br><span class="line"><span class="comment">% 参见课件Section2.2最后一块的伪代码</span></span><br><span class="line"><span class="comment">% 换句话说，如果我们使用批量梯度下降法去最小化参数，</span></span><br><span class="line"><span class="comment">% W1权值的更新应该是W1:= W1-alpha*W1grad, 而W2,b1,b2也类似</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% J_sparse(W,b)共有三项</span></span><br><span class="line">Jcost = <span class="number">0</span>; <span class="comment">% 样本均方误差项</span></span><br><span class="line">Jweight = <span class="number">0</span>; <span class="comment">% 权重衰减项</span></span><br><span class="line">Jsparse = <span class="number">0</span>; <span class="comment">% 稀疏性惩罚项</span></span><br><span class="line">[n m] = <span class="built_in">size</span>(data); <span class="comment">% n表示样本特征数，m表示样本个数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 前向算法计算各神经网络节点的线性组合值和active值</span></span><br><span class="line">z2 = W1*data + <span class="built_in">repmat</span>(b1, <span class="number">1</span>, m); <span class="comment">% 第二层输入值，即隐藏层中各节点输入值矩阵，repmat函数将矩阵进行重复并组合起来</span></span><br><span class="line">a2 = sigmoid(z2); <span class="comment">% 第二层输出值（激活值），注意sigmoid函数在本文件164行定义</span></span><br><span class="line">z3 = W2*a2 + <span class="built_in">repmat</span>(b2, <span class="number">1</span>, m); <span class="comment">% 第三层输入值</span></span><br><span class="line">a3 = sigmoid(z3); <span class="comment">% 输出层各节点输出值（激活值）矩阵</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算样本均方误差项</span></span><br><span class="line">Jcost = (<span class="number">0.5</span>/m)*sum(sum((a3-data).^<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算权重衰减项</span></span><br><span class="line">Jweight = (<span class="number">1</span>/<span class="number">2</span>)*(sum(sum(W1.^<span class="number">2</span>)) + sum(sum(W2.^<span class="number">2</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算稀疏性惩罚项</span></span><br><span class="line">rho = (<span class="number">1</span>/m).*sum(a2, <span class="number">2</span>); <span class="comment">% 求出教程中的rho，即隐藏层（第二层）神经元的平均活跃度，</span></span><br><span class="line">                                        <span class="comment">% 在训练集上取隐藏层每个节点输出值的平均</span></span><br><span class="line">Jsparse = sum(sparsityParam .* <span class="built_in">log</span>(sparsityParam ./ rho) + ...</span><br><span class="line">              (<span class="number">1</span>-sparsityParam) .* <span class="built_in">log</span>((<span class="number">1</span>-sparsityParam) ./ (<span class="number">1</span> - rho )));</span><br><span class="line">          </span><br><span class="line"><span class="comment">% 代价函数总表达式</span></span><br><span class="line">cost = Jcost+lambda*Jweight+<span class="built_in">beta</span>*Jsparse;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算残差</span></span><br><span class="line">d3 = -(data-a3).*sigmoidInv(z3); <span class="comment">% 输出层（第三层）残差，sigmoidInv为sigmoid导数，在第170行有定义</span></span><br><span class="line">sterm = <span class="built_in">beta</span>*(-sparsityParam./rho+(<span class="number">1</span>-sparsityParam)./(<span class="number">1</span>-rho)); <span class="comment">% 隐藏层稀疏性规则项</span></span><br><span class="line">d2 = (W2'*d3+<span class="built_in">repmat</span>(sterm, <span class="number">1</span>, m)).*sigmoidInv(z2); <span class="comment">% 隐藏层（第二层）残差，计算时要加入稀疏性规则项</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算W1grad</span></span><br><span class="line">W1grad = W1grad+d2*data';</span><br><span class="line">W1grad = (<span class="number">1</span>/m).*W1grad+lambda*W1;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算W2grad</span></span><br><span class="line">W2grad = W2grad+d3*a2';</span><br><span class="line">W2grad = (<span class="number">1</span>/m).*W2grad+lambda*W2;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算b1grad</span></span><br><span class="line">b1grad = b1grad + sum(d2, <span class="number">2</span>); <span class="comment">% 注意b的偏导数是一个向量，因此要把一行的值累加起来</span></span><br><span class="line">b1grad = (<span class="number">1</span>/m)*b1grad;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 计算b2grad</span></span><br><span class="line">b2grad = b2grad + sum(d3, <span class="number">2</span>);</span><br><span class="line">b2grad = (<span class="number">1</span>/m)*b2grad;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%-------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% After computing the cost and gradient, we will convert the gradients back</span></span><br><span class="line"><span class="comment">% to a vector format (suitable for minFunc).  Specifically, we will unroll</span></span><br><span class="line"><span class="comment">% your gradient matrices into a vector.</span></span><br><span class="line"></span><br><span class="line">grad = [W1grad(:) ; W2grad(:) ; b1grad(:) ; b2grad(:)]; <span class="comment">%将各矩阵按列首尾相接成为一个列向量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%-------------------------------------------------------------------</span></span><br><span class="line"><span class="comment">% Here's an implementation of the sigmoid function, which you may find useful</span></span><br><span class="line"><span class="comment">% in your computation of the costs and the gradients.  This inputs a (row or</span></span><br><span class="line"><span class="comment">% column) vector (say (z1, z2, z3)) and returns (f(z1), f(z2), f(z3)). </span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 定义sigmoid函数</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sigm</span> = <span class="title">sigmoid</span><span class="params">(x)</span></span></span><br><span class="line">  </span><br><span class="line">    sigm = <span class="number">1</span> ./ (<span class="number">1</span> + <span class="built_in">exp</span>(-x));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% sigmoid函数求导</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">sigmInv</span> = <span class="title">sigmoidInv</span><span class="params">(x)</span></span></span><br><span class="line">    </span><br><span class="line">    sigmInv = sigmoid(x).*(<span class="number">1</span>-sigmoid(x)); </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% computeNumericalGradient.m</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">numgrad</span> = <span class="title">computeNumericalGradient</span><span class="params">(J, theta)</span></span></span><br><span class="line"><span class="comment">% numgrad = computeNumericalGradient(J, theta)</span></span><br><span class="line"><span class="comment">% theta: a vector of parameters</span></span><br><span class="line"><span class="comment">% J: a function that outputs a real-number. Calling y = J(theta) will return the</span></span><br><span class="line"><span class="comment">% function value at theta. </span></span><br><span class="line">  </span><br><span class="line"><span class="comment">% Initialize numgrad with zeros</span></span><br><span class="line">numgrad = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta));</span><br><span class="line"></span><br><span class="line"><span class="comment">%% ---------- YOUR CODE HERE --------------------------------------</span></span><br><span class="line"><span class="comment">% Instructions: </span></span><br><span class="line"><span class="comment">% Implement numerical gradient checking, and return the result in numgrad.  </span></span><br><span class="line"><span class="comment">% (See Section 2.3 of the lecture notes.)</span></span><br><span class="line"><span class="comment">% You should write code so that numgrad(i) is (the numerical approximation to) the </span></span><br><span class="line"><span class="comment">% partial derivative of J with respect to the i-th input argument, evaluated at theta.  </span></span><br><span class="line"><span class="comment">% I.e., numgrad(i) should be the (approximately) the partial derivative of J with </span></span><br><span class="line"><span class="comment">% respect to theta(i).</span></span><br><span class="line"><span class="comment">%                </span></span><br><span class="line"><span class="comment">% Hint: You will probably want to compute the elements of numgrad one at a time. </span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 注释译文</span></span><br><span class="line"><span class="comment">% 指导：</span></span><br><span class="line"><span class="comment">% 实现数值上的梯度检验，并将结果返回到numgrad中</span></span><br><span class="line"><span class="comment">% （查看课程笔记的Section 2.3）</span></span><br><span class="line"><span class="comment">% 你应当写代码使numgrad(i)是J关于第i个输入参数的偏导数的近似值，输入参数为theta</span></span><br><span class="line"><span class="comment">% 也就是说，numgrad(i)应该近似是J关于theta(i)的偏导数</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% 提示：你很可能希望一次计算一个numgrad中的元素</span></span><br><span class="line"></span><br><span class="line">epsilon = <span class="number">1e-4</span>;</span><br><span class="line">n = <span class="built_in">size</span>(theta, <span class="number">1</span>); <span class="comment">% n为向量theta的维度</span></span><br><span class="line">E = <span class="built_in">eye</span>(n); <span class="comment">% 函数eye(n)用来创建n维的单位矩阵</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n</span><br><span class="line">    delta = E(:, <span class="built_in">i</span>)*epsilon;</span><br><span class="line">    numgrad(<span class="built_in">i</span>) = (J(theta+delta)-J(theta-delta))/(<span class="number">2.0</span>*epsilon);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">%% ---------------------------------------------------------------</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
  
	</div>
		<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="https://wanz2.github.io/2017/04/29/深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习/" data-title="深度学习笔记：稀疏自编码器（4）——稀疏自编码器代码练习 | Aaron Wu" data-tsina="undefined" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/04/29/深度学习笔记：主成分分析（PCA）——标准化、协方差、相关系数和协方差矩阵/" title="深度学习笔记：主成分分析（PCA）——标准化、协方差、相关系数和协方差矩阵">
  <strong>上一篇：</strong><br/>
  <span>
  深度学习笔记：主成分分析（PCA）——标准化、协方差、相关系数和协方差矩阵</span>
</a>
</div>


<div class="next">
<a href="/2017/04/29/深度学习笔记：稀疏自编码器（3）——稀疏自编码算法/"  title="深度学习笔记：稀疏自编码器（3）——稀疏自编码算法">
 <strong>下一篇：</strong><br/> 
 <span>深度学习笔记：稀疏自编码器（3）——稀疏自编码算法
</span>
</a>
</div>

</nav>

	

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">文章目录</strong>
 
 
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/Python/Docker/" title="Docker">Docker<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Java/" title="Java">Java<sup>9</sup></a></li>
		  
		
		  
			<li><a href="/categories/Linux/" title="Linux">Linux<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Python/" title="Python">Python<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Web开发/" title="Web开发">Web开发<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/图像处理/" title="图像处理">图像处理<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/数据库/" title="数据库">数据库<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/数据结构与算法/" title="数据结构与算法">数据结构与算法<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/机器学习/" title="机器学习">机器学习<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/系统架构/" title="系统架构">系统架构<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/经验心得/" title="经验心得">经验心得<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/设计模式/" title="设计模式">设计模式<sup>4</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Java/" title="Java">Java<sup>11</sup></a></li>
			
		
			
				<li><a href="/tags/机器学习/" title="机器学习">机器学习<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/设计模式/" title="设计模式">设计模式<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/经验心得/" title="经验心得">经验心得<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/数据库/" title="数据库">数据库<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Java虚拟机/" title="Java虚拟机">Java虚拟机<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/泛型/" title="泛型">泛型<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Web开发/" title="Web开发">Web开发<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Python/" title="Python">Python<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Docker/" title="Docker">Docker<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/数据结构与算法/" title="数据结构与算法">数据结构与算法<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/系统架构/" title="系统架构">系统架构<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/框架/" title="框架">框架<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/图像处理/" title="图像处理">图像处理<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Linux/" title="Linux">Linux<sup>1</sup></a></li>
			
		
		</ul>
</div>


  
  <div class="tagcloudlist">
    <p class="asidetitle">标签云</p>
    <div class="tagcloudlist clearfix">
       <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Java/" style="font-size: 20px;">Java</a> <a href="/tags/Java虚拟机/" style="font-size: 10px;">Java虚拟机</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Web开发/" style="font-size: 10px;">Web开发</a> <a href="/tags/图像处理/" style="font-size: 10px;">图像处理</a> <a href="/tags/数据库/" style="font-size: 10px;">数据库</a> <a href="/tags/数据结构与算法/" style="font-size: 10px;">数据结构与算法</a> <a href="/tags/机器学习/" style="font-size: 17.5px;">机器学习</a> <a href="/tags/框架/" style="font-size: 10px;">框架</a> <a href="/tags/泛型/" style="font-size: 10px;">泛型</a> <a href="/tags/系统架构/" style="font-size: 10px;">系统架构</a> <a href="/tags/经验心得/" style="font-size: 12.5px;">经验心得</a> <a href="/tags/设计模式/" style="font-size: 15px;">设计模式</a>
    </div>
  </div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="http://coolshell.cn" target="_blank" title="COOLSHELL">COOLSHELL</a>
            
          </li>
        
          <li>
            
            	<a href="http://qinjiangbo.com" target="_blank" title="心灵港">心灵港</a>
            
          </li>
        
          <li>
            
            	<a href="http://www.whu.edu.cn" target="_blank" title="武汉大学">武汉大学</a>
            
          </li>
        
    </ul>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> I&#39;m Aaron Wu. <br/>
			This is my blog.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		<a href="https://github.com/Wanz2" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:wuwenan_6@foxmail.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2020 
		
		<a href="/about" target="_blank" title="Aaron Wu">Aaron Wu</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>









<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1261991776'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s23.cnzz.com/z_stat.php%3Fid%3D1261991776' type='text/javascript'%3E%3C/script%3E"));</script>

<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End --><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
</html>
