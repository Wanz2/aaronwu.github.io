<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>深度学习笔记：稀疏自编码器（1）————神经元与神经网络 | Aaron Wu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="&amp;emsp;&amp;emsp;笔者在很久以前就已经学习过UFLDL深度学习教程中的稀疏自编码器，近期需要用到的时候发现有些遗忘，温习了一遍之后决定在这里做一下笔记，本文不是对神经元与神经网络的介绍，而是笔者学习之后做的归纳和整理，打算分为几篇记录。详细教程请见UFLDL教程，看完教程之后再来看本文可能会更加清晰。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习笔记：稀疏自编码器（1）————神经元与神经网络">
<meta property="og:url" content="https://wanz2.github.io/2017/04/29/深度学习笔记：稀疏自编码器（1）————神经元与神经网络/index.html">
<meta property="og:site_name" content="Aaron Wu">
<meta property="og:description" content="&amp;emsp;&amp;emsp;笔者在很久以前就已经学习过UFLDL深度学习教程中的稀疏自编码器，近期需要用到的时候发现有些遗忘，温习了一遍之后决定在这里做一下笔记，本文不是对神经元与神经网络的介绍，而是笔者学习之后做的归纳和整理，打算分为几篇记录。详细教程请见UFLDL教程，看完教程之后再来看本文可能会更加清晰。">
<meta property="og:image" content="http://odnk9as2f.bkt.clouddn.com/300px-SingleNeuron.png">
<meta property="og:image" content="http://odnk9as2f.bkt.clouddn.com/400px-Network331.png">
<meta property="og:updated_time" content="2017-05-07T11:10:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习笔记：稀疏自编码器（1）————神经元与神经网络">
<meta name="twitter:description" content="&amp;emsp;&amp;emsp;笔者在很久以前就已经学习过UFLDL深度学习教程中的稀疏自编码器，近期需要用到的时候发现有些遗忘，温习了一遍之后决定在这里做一下笔记，本文不是对神经元与神经网络的介绍，而是笔者学习之后做的归纳和整理，打算分为几篇记录。详细教程请见UFLDL教程，看完教程之后再来看本文可能会更加清晰。">
<meta name="twitter:image" content="http://odnk9as2f.bkt.clouddn.com/300px-SingleNeuron.png">
  
    <link rel="alternate" href="/atom.xml" title="Aaron Wu" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Aaron Wu</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Strive for excellence, not perfection.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://wanz2.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-深度学习笔记：稀疏自编码器（1）————神经元与神经网络" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/29/深度学习笔记：稀疏自编码器（1）————神经元与神经网络/" class="article-date">
  <time datetime="2017-04-29T15:38:52.000Z" itemprop="datePublished">2017-04-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      深度学习笔记：稀疏自编码器（1）————神经元与神经网络
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>&emsp;&emsp;笔者在很久以前就已经学习过UFLDL深度学习教程中的稀疏自编码器，近期需要用到的时候发现有些遗忘，温习了一遍之后决定在这里做一下笔记，本文不是对神经元与神经网络的介绍，而是笔者学习之后做的归纳和整理，打算分为几篇记录。详细教程请见<a href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B" target="_blank" rel="external">UFLDL教程</a>，看完教程之后再来看本文可能会更加清晰。  </p>
<a id="more"></a>
<h2 id="0-本文中使用的符号一览及本文中的一些约定"><a href="#0-本文中使用的符号一览及本文中的一些约定" class="headerlink" title="0. 本文中使用的符号一览及本文中的一些约定"></a>0. 本文中使用的符号一览及本文中的一些约定</h2><table>
<thead>
<tr>
<th>符号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>$x$</td>
<td>输入值向量，为$n$维</td>
</tr>
<tr>
<td>$x_i$</td>
<td>第$i$个输入值</td>
</tr>
<tr>
<td>$f()$</td>
<td>神经元激活函数，本文中为sigmoid函数，即$f(z)=\frac{1}{1+e^{(-z)}}$</td>
</tr>
<tr>
<td>$W_{ij}^{(l)}$</td>
<td>神经网络中第$l$层第$j$个神经元和第$l+1$层第$i$个神经元连线上的权值</td>
</tr>
<tr>
<td>$b_i^{(l)}$</td>
<td>第$l+1$层第$i$个神经元输入的偏置项</td>
</tr>
<tr>
<td>$n_l$</td>
<td>神经网络的总层数</td>
</tr>
<tr>
<td>$L_l$</td>
<td>第$l$层，则输入层为$L<em>1$，输出层为$L</em>{nl}$</td>
</tr>
<tr>
<td>$s_l$</td>
<td>第$l$层的节点数（不包括偏置单元）</td>
</tr>
<tr>
<td>$a_i^{l}$</td>
<td>第$l$层第$i$单元的激活值（输出值），当$l=1$时，$a_i^{(1)}=x_i$</td>
</tr>
<tr>
<td>$z_i^{(l)}$</td>
<td>第$l$层第$i$单元输入加权和（包括偏置单元），有$a_i^{(l)}=f(z_i^{(l)})$</td>
</tr>
<tr>
<td>$h_{W,b}(x)$</td>
<td>输入值为$x$，神经网络中权值和偏置项分别为$W,b$的情况下的输出值</td>
</tr>
</tbody>
</table>
<p>&emsp;<br><strong>约定</strong> 本文中将函数$f()$以及偏导数$f’()$进行了针对向量参数的扩展，即：<br>$f([z_1,z_2,z_3])=[f(z_1),f(z_2),f(z_3)]$<br>$f’([z_1,z_2,z_3])=[f’(z_1),f’(z_2),f’(z_3)]$</p>
<h2 id="1-神经元"><a href="#1-神经元" class="headerlink" title="1. 神经元"></a>1. 神经元</h2><h3 id="1-1-神经元定义"><a href="#1-1-神经元定义" class="headerlink" title="1.1 神经元定义"></a>1.1 神经元定义</h3><p>&emsp;&emsp;神经元是以$x$为输入，$h_{W,b}$为输出的运算单元，如图所示：<br><img src="http://odnk9as2f.bkt.clouddn.com/300px-SingleNeuron.png" alt="此处输入图片的描述"></p>
<h3 id="1-2-激活函数"><a href="#1-2-激活函数" class="headerlink" title="1.2 激活函数"></a>1.2 激活函数</h3><p>&emsp;&emsp;通常在神经元中输出值为0表示神经元被激活，输出为1表示神经元被抑制，而计算输出值所用的函数则被称为“激活函数”。本文中所用的激活函数为sigmoid函数：$f(z)=\frac{1}{1+e^{(-z)}}$，其定义域为$(-\infty,+\infty)$，值域为$(0,1)$。以上图神经元为例，假设第$i$个输入值与神经元连线上的权重为$w_1$，输入值偏置项为$b<em>1$，在单个神经元中，有$h</em>{W,b}(x)=f(x_1w_1+x_2w_2+x_3w_3+b_1)$。观察可以发现，单个神经元就是一个<strong>逻辑回归</strong>模型。</p>
<h2 id="2-神经网络"><a href="#2-神经网络" class="headerlink" title="2. 神经网络"></a>2. 神经网络</h2><h3 id="2-1-神经网络定义"><a href="#2-1-神经网络定义" class="headerlink" title="2.1 神经网络定义"></a>2.1 神经网络定义</h3><p>&emsp;&emsp;神经网络就是将多个神经元连在一起，前一层神经元的输出就是后一层神经元的输入，下图是一个三层神经网络：<br><img src="http://odnk9as2f.bkt.clouddn.com/400px-Network331.png" alt="此处输入图片的描述"><br>以上图为例，最左边一层为<strong>输入层</strong>，中间一层为<strong>隐藏层</strong>，最右边一层为<strong>输出层</strong>。</p>
<h3 id="2-2-前向传播"><a href="#2-2-前向传播" class="headerlink" title="2.2 前向传播"></a>2.2 前向传播</h3><p>&emsp;&emsp;前向传播就是使用输入值$x$通过神经网络计算出输出值$h_{W,b}(x)$的过程。以上图神经网络为例，前向传播中每一个神经元输入输出过程如下：<br>令$z_i^{(l)}$表示第$l$层第$i$个神经元的输入值，有<br>$z<em>i^{(l+1)}=W</em>{i1}^{(l)}x<em>1+W</em>{i2}^{(l)}x<em>2+W</em>{i3}^{(l)}x_3+b_i^{(l)}$ （这里其实是一个<strong>线性回归</strong>模型）<br>$a_i^{(l)}=f(z<em>i^{(l)})$<br>$h</em>{W,b}(x)=a_1^{(3)}$</p>
<h3 id="2-3-前向传播的矩阵表示"><a href="#2-3-前向传播的矩阵表示" class="headerlink" title="2.3 前向传播的矩阵表示"></a>2.3 前向传播的矩阵表示</h3><p>&emsp;&emsp;本文中扩展了函数$f()$针对向量参数的约定，即有：<br>$f([z_1,z_2,z_3])=[f(z_1),f(z_2),f(z<em>3)]$<br>则上图神经网络中的前向传播过程可表示为：<br>$z^{(2)}=W^{(1)}x+b^{(1)}$<br>$a^{(2)}=f(z^{(2)})$<br>$z^{(3)}=W^{(2)}a^{(2)}+b^{(2)}$<br>$h</em>{W,b}(x)=f(z^{(3)})$<br>下面以$z^{(2)}=W^{(1)}x+b^{(1)}$为例，展开一下具体的矩阵表示，其他式子略：<br>$\begin{pmatrix}z_1^{(2)}\ z_2^{(2)}\ z<em>3^{(2)}\end{pmatrix}=\begin{pmatrix}W</em>{11}^{(1)}&amp; W<em>{12}^{(1)} &amp; W</em>{13}^{(1)}\ W<em>{21}^{(1)}&amp; W</em>{22}^{(1)} &amp; W<em>{23}^{(1)}\ W</em>{31}^{(1)}&amp; W<em>{32}^{(1)} &amp; W</em>{33}^{(1)} \end{pmatrix} \begin{pmatrix} x_1\ x_2\ x_3 \end{pmatrix}+ \begin{pmatrix}b_1^{(1)} \ b_2^{(1)}\ b_3^{(1)} \end{pmatrix}$<br>最终，神经网络前向传播过程可表示为：<br>$z^{(l+1)}=W^{(l)}a^{(l)}+b^{(l)}$<br>$a^{(l+1)}=f(z^{(l+1)})$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wanz2.github.io/2017/04/29/深度学习笔记：稀疏自编码器（1）————神经元与神经网络/" data-id="cj2elphz8000dqy0vcuf6m0eq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/04/29/深度学习笔记：稀疏自编码器（2）——反向传导/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          深度学习笔记：稀疏自编码器（2）——反向传导
        
      </div>
    </a>
  
  
    <a href="/2017/04/06/压缩感知算法原理/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">压缩感知算法原理</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VMWare/">VMWare</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/图像处理/">图像处理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/VMWare/" style="font-size: 10px;">VMWare</a> <a href="/tags/图像处理/" style="font-size: 10px;">图像处理</a> <a href="/tags/机器学习/" style="font-size: 20px;">机器学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/05/05/Java——对象的序列化与反序列化（1）/">Java——对象的序列化与反序列化（1）</a>
          </li>
        
          <li>
            <a href="/2017/05/04/Java——JDBC执行SQL语句的两种方式/">Java——JDBC执行SQL语句的两种方式</a>
          </li>
        
          <li>
            <a href="/2017/04/30/Java——利用Collections.sort()对List排序/">Java——利用Collections.sort()对泛型为String的List排序</a>
          </li>
        
          <li>
            <a href="/2017/04/30/Java——重写equals-方法/">Java——重写equals()方法</a>
          </li>
        
          <li>
            <a href="/2017/04/29/深度学习笔记：主成分分析（PCA）——标准化、协方差、相关系数和协方差矩阵/">深度学习笔记：主成分分析（PCA）——标准化、协方差、相关系数和协方差矩阵</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Aaron Wu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>